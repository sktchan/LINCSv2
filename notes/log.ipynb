{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7cedc9",
   "metadata": {},
   "source": [
    "<< to keep track of changes made, prev. ideas, etc. >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f3a3fe",
   "metadata": {},
   "source": [
    "apr 8, 2025 - flux_transf.jl\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b69bd4",
   "metadata": {},
   "source": [
    "this is what was used for original 2D input into mhsa (which doesn't work, needs (k, q, v) input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function (tf::Transf)(input::Float32Matrix2DType) # input is features (978) x batch\n",
    "    sa_out = tf.mhsa(tf.norm_1(input)) # OG paper states norm after sa, but norm before sa is more common?\n",
    "    # x = input + tf.dropout(sa_out)\n",
    "    x = input + sa_out\n",
    "    mlp_out = tf.mlp(tf.norm_2(x))\n",
    "    # x = x + tf.dropout(mlp_out)\n",
    "    x = x + mlp_out\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c617cb3",
   "metadata": {},
   "source": [
    "apr 11, 2025 - flux_transf.jl\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc0b2b",
   "metadata": {},
   "source": [
    "using a for loop and replacing a matrix of -1 rather than making a copy is much faster! \n",
    "using the function below (commented parts) is a little easier to read, but the current impl. in the file is much more efficient.\n",
    "tmp: if we were to replace the -1s with not the gene names (Str) but with the ranking ints themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sort_gene(expr)\n",
    "    # data_ranked = Matrix{Int}(undef, size(expr))\n",
    "    data_ranked = fill(-1, size(expr))\n",
    "    # gs = Symbol.(gene_symbols)\n",
    "    n, m = size(expr)\n",
    "    p = Vector{Int}(undef, n)\n",
    "    # tmp = sortperm(expr[:, 1])\n",
    "\n",
    "    for j in 1:m\n",
    "        e = view(expr, :, j)\n",
    "        sortperm!(p, e, rev=true)\n",
    "        # data_ranked[!, j] = gs[tmp]\n",
    "\n",
    "        for i in 1:n\n",
    "            data_ranked[i, j] = p[i]\n",
    "        end\n",
    "    end\n",
    "    return data_ranked\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec85ec5f",
   "metadata": {},
   "source": [
    "GENES AS TOKENS gene-gene interactions (sequence length as len(genes))\n",
    "- each gene is a position in your sequence\n",
    "- each token's embedding contains information about that gene's ranking across samples\n",
    "- the sequence length equals the number of genes you're considering\n",
    "SAMPLES AS TOKENS sample-sample interactions (sequence length as len(samples))\n",
    "- each sample is a position in your sequence\n",
    "- each token's embedding contains the gene ranking information for that sample\n",
    "- the sequence length equals the number of samples\n",
    "TECHNICAL CONSIDERATIONS\n",
    "transformers struggle with very long sequences, so if we have many more genes than samples, using samples as tokens may be more computationally feasible\n",
    "self-attention complexity grows quadratically with sequence length\n",
    "which dimension has more examples to learn from?\n",
    "\n",
    "good option: Flux.MultiHeadAttention((64, 64, 64) => (64, 64) => 64, nheads=1), can incr nheads later\n",
    "- q, k, v input dim should all be the same if data type is the same or we aren't doing encoder-decoder\n",
    "- middle dimensions should also be the same unless we want to reduce computational complexity in the middle\n",
    "- output can also be the same unless we want to do ft compression or expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8df65",
   "metadata": {},
   "source": [
    "may 1, 2025 - masked loss fxn - DONE\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec7019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_masked(model, x, y_masked)\n",
    "    logits = model(x)  # (n_classes, seq_len, batch_size)\n",
    "    logits = permutedims(logits, (2, 3, 1))  # seq_len × batch_size × n_classes (to match targets)\n",
    "    logits = reshape(logits, :, n_classes)   # (seq_len * batch_size) × n_classes\n",
    "\n",
    "    y_masked_flat = vec(y_masked) # flatten\n",
    "    # only keep where y_masked != -100\n",
    "    mask = y_masked_flat .!= -100\n",
    "    logits_masked = (logits[mask, :])'\n",
    "    targets_masked = y_masked_flat[mask]\n",
    "    y_oh = Flux.onehotbatch(targets_masked, 1:n_classes)\n",
    "\n",
    "    return Flux.logitcrossentropy(logits_masked, y_oh)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f723c",
   "metadata": {},
   "source": [
    "may 27, 2025 - training fxn with masked values for accuracy - DONE\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ffb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss(model, x, y)\n",
    "    logits = model(x)  # (n_classes, seq_len, batch_size)\n",
    "    logits_flat = reshape(logits, size(logits, 1), :) # (n_classes, seq_len*batch_size)\n",
    "    y_flat = vec(y) # (seq_len*batch_size) column vec\n",
    "\n",
    "    mask = y_flat .!= -100 # bit vec, where sum = n_masked\n",
    "    logits_masked = logits_flat[:, mask] # (n_classes, n_masked)\n",
    "    y_masked = y_flat[mask] # (n_masked) column vec\n",
    "\n",
    "    y_oh = Flux.onehotbatch(y_masked, 1:n_classes) # (n_classes, n_masked)\n",
    "    return Flux.logitcrossentropy(logits_masked, y_oh) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505e3725",
   "metadata": {},
   "source": [
    "could return logits_masked and y_masked as well, then do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_masked = Flux.onecold(logits_masked)\n",
    "preds_masked_cpu = preds_masked |> cpu\n",
    "preds_masked_cpu .== y_masked\n",
    "accuracy = sum(preds_masked_cpu .== y_masked) / length(y_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604ef7db",
   "metadata": {},
   "source": [
    "may 29, 2025 - sparse matrices\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in train loop, instead of y_gpu:\n",
    "y_batch_sparse = get_sparse_batch(y_train_masked, start_idx, end_idx)\n",
    "\n",
    "using SparseArrays\n",
    "\n",
    "function mask_input_sparse(X::Matrix{Int64}; mask_ratio=0.10)\n",
    "    X_masked = copy(X)\n",
    "    # Create sparse matrix for labels\n",
    "    I_indices = Int[]  # row indices\n",
    "    J_indices = Int[]  # column indices  \n",
    "    values = Int16[]   # actual gene indices\n",
    "    \n",
    "    for j in 1:size(X, 2)\n",
    "        num_masked = ceil(Int, size(X, 1) * mask_ratio)\n",
    "        mask_positions = randperm(size(X, 1))[1:num_masked]\n",
    "        \n",
    "        for pos in mask_positions\n",
    "            push!(I_indices, pos)\n",
    "            push!(J_indices, j)\n",
    "            push!(values, X[pos, j])  # original gene index\n",
    "            \n",
    "            X_masked[pos, j] = MASK_ID\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    y_sparse = sparse(I_indices, J_indices, values, size(X)...)\n",
    "    return X_masked, y_sparse\n",
    "end\n",
    "\n",
    "X_train_masked, y_train_masked = mask_input_sparse(X_train)\n",
    "X_test_masked, y_test_masked = mask_input_sparse(X_test)\n",
    "\n",
    "function loss_sparse(model, x, y_sparse_batch, mode)\n",
    "    logits = model(x)  # (n_classes, seq_len, batch_size)\n",
    "    \n",
    "    rows_cpu, cols_cpu, vals_cpu = findnz(y_sparse_batch)\n",
    "    \n",
    "    if isempty(rows_cpu)\n",
    "        return 0.0f0\n",
    "    end\n",
    "    \n",
    "    rows_gpu = cu(rows_cpu)\n",
    "    cols_gpu = cu(cols_cpu) \n",
    "    vals_gpu = cu(vals_cpu)\n",
    "    \n",
    "    batch_size = size(logits, 3)\n",
    "    seq_len = size(logits, 2)\n",
    "    \n",
    "    linear_indices = (cols_gpu .- 1) .* seq_len .+ rows_gpu\n",
    "    \n",
    "    logits_reshaped = reshape(logits, size(logits, 1), :) # (n_classes, seq_len * batch_size)\n",
    "    masked_logits = logits_reshaped[:, linear_indices]  # (n_classes, n_masked)\n",
    "    \n",
    "    y_oh = Flux.onehotbatch(vals_gpu, 1:n_classes)\n",
    "    \n",
    "    if mode == \"train\"\n",
    "        return Flux.logitcrossentropy(masked_logits, y_oh)\n",
    "    elseif mode == \"test\"\n",
    "        return Flux.logitcrossentropy(masked_logits, y_oh), masked_logits, vals_gpu\n",
    "    end\n",
    "end\n",
    "\n",
    "function get_sparse_batch(y_sparse, start_idx, end_idx)\n",
    "    rows, cols, vals = findnz(y_sparse[:, start_idx:end_idx])\n",
    "    cols_adjusted = cols\n",
    "    batch_sparse = sparse(rows, cols_adjusted, vals, size(y_sparse, 1), end_idx - start_idx + 1)\n",
    "    \n",
    "    return batch_sparse\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e28e3c",
   "metadata": {},
   "source": [
    "** with the above code, 11mins 1 epoch (see github @ this time for other params) VS. 11mins 1 epoch for dense representations.\n",
    "can revisit later if there is found to be memory bottlenecks @ matrix operations, however for now the dense is sufficient b/c:\n",
    "https://www.reddit.com/r/Julia/comments/108g5ou/when_is_it_worth_working_with_sparse_matrices/\n",
    "https://medium.com/data-science/sparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71\n",
    "- above state that there should be about 1% or less sparsity for GPU sparse matrices to be efficient\n",
    "- due to the need to repeatedly transfer data between CPU and GPU and sparse slicing operations in batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29a6d02",
   "metadata": {},
   "source": [
    "june 12, 2025 trying dynamic masking;\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981827d4",
   "metadata": {},
   "source": [
    "**RoBERTa shows that masking a different subset every epoch already helps, \n",
    "but recent work finds that decreasing the rate during training is even better \n",
    "(to try later!!! aka scheduler)\n",
    "\n",
    "***dynamic on the train, static on the test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6fe77",
   "metadata": {},
   "source": [
    "jun 16, 2025 - tried to only mask 1 position; if it can't predict just the missing # from 1-978, then it's dumb.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb01a0e7",
   "metadata": {},
   "source": [
    "- the issue here might be that the test mask is different from the train mask; \n",
    "thus w/o dynamic masking, the train learns a single value each time, and the test provides a new value not seen before..?\n",
    "\n",
    "also trying to profile the memory b/c 25min per epoch is way too long;\n",
    "Profile.Allocs.@profile sample_rate=1 begin/end is taking wayyyyy too long too (~3hrs so far) to run in the REPL.. maybe\n",
    "is there another way?\n",
    "\n",
    "*also what takes 26h on kraken takes 4h on smaug...\n",
    "there seems to be slightly better trainval loss using higher embed dim --> try higher dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5c7136",
   "metadata": {},
   "source": [
    "jun 20, 2025 - based on the results from smaug, 2025-06-19_21-48\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107b8d3",
   "metadata": {},
   "source": [
    "there seems to be an issue with learning diff masked tokens (1 per sample) across the whole dataset\n",
    "it works if we want to do the same say, 5 masks across the whole dataset (albeit at only a 84% accuracy for some reason)\n",
    "1. double check masking function - ensure that it is correct\n",
    "2. scale up learning rate..? not sure what else to do here\n",
    "the training masks have sufficinet examples to learn from i think (60 per label) so what is going wrong?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1b0b2",
   "metadata": {},
   "source": [
    "jul 24, 2025 - speed/memory optimization\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac89a91",
   "metadata": {},
   "source": [
    "- lux.jl is equivalent of jax in python (uses xla backend)\n",
    "- unrelated, but wb a splitted data struct..? (what lea uses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d657f",
   "metadata": {},
   "source": [
    "jul 25, 2025 - exp masking...?\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d397d7bf",
   "metadata": {},
   "source": [
    "1. can you embed counts? how would that work\n",
    "2. needs to be a regression output (1 value) rather than a vector of probabilities per class?\n",
    "3. loss needs to be defined differently\n",
    "4. should it just be a dense network? does MHA work on "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae0d249",
   "metadata": {},
   "source": [
    "jul 30, 2025 - exp masking\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a574d2d",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Dense layer instead of Flux.Embedding \n",
    "    - this is b/c below static vs. dynamic Embeddings\n",
    "    - dense layer stores f(x) = Wx + b, where W and b are learned during training for all genes/samples\n",
    "    - W is a matrix, b is a vector and these are multiplied by the input x to output f(x) the embeddign vector \n",
    "- MHA still works with non-tokenized values!\n",
    "    - model learns the meaning of the expression levels rather than the gene identity/ranks?\n",
    "- mask token should be 0.0 after normalizing input\n",
    "- MSE loss\n",
    "- regression output to 1 value\n",
    "\n",
    "Static Embeddings (like Word2Vec or Flux.Embedding): \n",
    "- The model learns one single, fixed vector for each unique token (e.g., the word \"bank\"). \n",
    "- The goal is to learn the meaning of the token itself by averaging its usage across thousands of different contexts.\n",
    "Dynamic Feature Representation (Your Model): \n",
    "- Your model does not learn a static vector for \"Gene 1\". \n",
    "- Instead, it learns a function that maps any given expression value to a vector representation.\n",
    "\n",
    "9PM - edit to exp masking\n",
    "- had to move loss calc to before model update - resulted in lower loss in test than train\n",
    "- change masking val to -1, apparenlty there are exp elvels of 0 in the original dataset?\n",
    "\n",
    "***shoudl make a plot similar to this scatter plot for check_error.jl file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93ac6e4",
   "metadata": {},
   "source": [
    "aug 1, 2025 - pre lab-meeting\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5948cb",
   "metadata": {},
   "source": [
    "***need to fix the logging of params for predstrues.csv and params.txt ; didn't save in the last couple runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbd354f",
   "metadata": {},
   "source": [
    "aug 4, 2025 - debug + seb loss issue\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0740cc47",
   "metadata": {},
   "source": [
    "- runnign on GPU 2 for indef run - rerun for seb\n",
    "- need to \n",
    "    1. figure out logging for test/train - is it diff than what's in the slides?\n",
    "    2. debug original structure typing\n",
    "    3. fix logging of params for predstrues.csv and params.txt when doing input comparison plots\n",
    "    4. fix progressbar for indef_run.jl (why is it out of 628, repeats for each epoch???)\n",
    "\n",
    "- 08-04 run is original test/loss definitions from creating 720ep graph\n",
    "- changed code to use mask_transf code in the while loop\n",
    "- so Flux.withgradient = 1st loss calc --> update! --> second loss calc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b97cb",
   "metadata": {},
   "source": [
    "aug 5, 2025 - debug\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b30855",
   "metadata": {},
   "source": [
    "original structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85463f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PosEnc\n",
    "    pe_matrix::CuArray{Float32,2}\n",
    "end\n",
    "\n",
    "function PosEnc(embed_dim::Int, max_len::Int) # max_len is usually maximum length of sequence but here it is just len(genes)\n",
    "    pe_matrix = Matrix{Float32}(undef, embed_dim, max_len)\n",
    "    for pos in 1:max_len, i in 1:embed_dim\n",
    "        angle = pos / (10000^(2*(div(i-1,2))/embed_dim))\n",
    "        if mod(i, 2) == 1\n",
    "            pe_matrix[i,pos] = sin(angle) # odd indices\n",
    "        else\n",
    "            pe_matrix[i,pos] = cos(angle) # even indices\n",
    "        end\n",
    "    end\n",
    "    return PosEnc(cu(pe_matrix))\n",
    "end\n",
    "\n",
    "Flux.@functor PosEnc\n",
    "\n",
    "function (pe::PosEnc)(input::Float32Matrix3DType)\n",
    "    seq_len = size(input,2)\n",
    "    return input .+ pe.pe_matrix[:,1:seq_len] # adds positional encoding to input embeddings\n",
    "end\n",
    "\n",
    "### building transformer section\n",
    "\n",
    "struct Transf\n",
    "    mha::Flux.MultiHeadAttention\n",
    "    att_dropout::Flux.Dropout\n",
    "    att_norm::Flux.LayerNorm # this is the normalization aspect\n",
    "    mlp::Flux.Chain\n",
    "    mlp_norm::Flux.LayerNorm\n",
    "end\n",
    "\n",
    "function Transf(\n",
    "    embed_dim::Int, \n",
    "    hidden_dim::Int; \n",
    "    n_heads::Int, \n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    mha = Flux.MultiHeadAttention((embed_dim, embed_dim, embed_dim) => (embed_dim, embed_dim) => embed_dim, \n",
    "                                    nheads=n_heads, \n",
    "                                    dropout_prob=dropout_prob\n",
    "                                    )\n",
    "\n",
    "    att_dropout = Flux.Dropout(dropout_prob)\n",
    "    \n",
    "    att_norm = Flux.LayerNorm(embed_dim)\n",
    "    \n",
    "    mlp = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => hidden_dim, gelu),\n",
    "        Flux.Dropout(dropout_prob),\n",
    "        Flux.Dense(hidden_dim => embed_dim),\n",
    "        Flux.Dropout(dropout_prob)\n",
    "        )\n",
    "    mlp_norm = Flux.LayerNorm(embed_dim)\n",
    "\n",
    "    return Transf(mha, att_dropout, att_norm, mlp, mlp_norm)\n",
    "end\n",
    "\n",
    "Flux.@functor Transf\n",
    "\n",
    "function (tf::Transf)(input::Float32Matrix3DType) # input shape: embed_dim × seq_len × batch_size\n",
    "    normed = tf.att_norm(input)\n",
    "    atted = tf.mha(normed, normed, normed)[1] # outputs a tuple (a, b)\n",
    "    att_dropped = tf.att_dropout(atted)\n",
    "    residualed = input + att_dropped\n",
    "    res_normed = tf.mlp_norm(residualed)\n",
    "\n",
    "    embed_dim, seq_len, batch_size = size(res_normed)\n",
    "    reshaped = reshape(res_normed, embed_dim, seq_len * batch_size) # dense layers expect 2D inputs\n",
    "    mlp_out = tf.mlp(reshaped)\n",
    "    mlp_out_reshaped = reshape(mlp_out, embed_dim, seq_len, batch_size)\n",
    "    \n",
    "    tf_output = residualed + mlp_out_reshaped\n",
    "    return tf_output\n",
    "end\n",
    "\n",
    "### full model as << ranked data --> token embedding --> position embedding --> transformer --> classifier head >>\n",
    "\n",
    "struct Model\n",
    "    embedding::Flux.Embedding\n",
    "    pos_encoder::PosEnc\n",
    "    pos_dropout::Flux.Dropout\n",
    "    transformer::Flux.Chain\n",
    "    classifier::Flux.Chain\n",
    "end\n",
    "\n",
    "function Model(;\n",
    "    input_size::Int,\n",
    "    embed_dim::Int,\n",
    "    n_layers::Int,\n",
    "    n_classes::Int,\n",
    "    n_heads::Int,\n",
    "    hidden_dim::Int,\n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    embedding = Flux.Embedding(input_size => embed_dim)\n",
    "\n",
    "    pos_encoder = PosEnc(embed_dim, input_size)\n",
    "\n",
    "    pos_dropout = Flux.Dropout(dropout_prob)\n",
    "\n",
    "    transformer = Flux.Chain(\n",
    "        [Transf(embed_dim, hidden_dim; n_heads, dropout_prob) for _ in 1:n_layers]...\n",
    "        )\n",
    "\n",
    "    classifier = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => embed_dim, gelu),\n",
    "        Flux.LayerNorm(embed_dim),\n",
    "        Flux.Dense(embed_dim => n_classes)\n",
    "        )\n",
    "\n",
    "    return Model(embedding, pos_encoder, pos_dropout, transformer, classifier)\n",
    "end\n",
    "\n",
    "Flux.@functor Model\n",
    "\n",
    "function (model::Model)(input::IntMatrix2DType)\n",
    "    embedded = model.embedding(input)\n",
    "    encoded = model.pos_encoder(embedded)\n",
    "    encoded_dropped = model.pos_dropout(encoded)\n",
    "    transformed = model.transformer(encoded_dropped)\n",
    "    # pooled = dropdims(mean(transformed; dims=2), dims=2)\n",
    "    logits_output = model.classifier(transformed)\n",
    "    return logits_output\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ae10f",
   "metadata": {},
   "source": [
    "re-typed structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaccda7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PosEnc{U<:AbstractMatrix}\n",
    "    pe_matrix::U\n",
    "end\n",
    "\n",
    "function PosEnc(embed_dim::Int, max_len::Int) # max_len is usually maximum length of sequence but here it is just len(genes)\n",
    "    pe_matrix = Matrix{Float32}(undef, embed_dim, max_len)\n",
    "    for pos in 1:max_len, i in 1:embed_dim\n",
    "        angle = pos / (10000^(2*(div(i-1,2))/embed_dim))\n",
    "        if mod(i, 2) == 1\n",
    "            pe_matrix[i,pos] = sin(angle) # odd indices\n",
    "        else\n",
    "            pe_matrix[i,pos] = cos(angle) # even indices\n",
    "        end\n",
    "    end\n",
    "    return PosEnc(pe_matrix)\n",
    "end\n",
    "\n",
    "Flux.@functor PosEnc\n",
    "\n",
    "function (pe::PosEnc)(input::Float32Matrix3DType)\n",
    "    seq_len = size(input,2)\n",
    "    return input .+ pe.pe_matrix[:,1:seq_len] # adds positional encoding to input embeddings\n",
    "end\n",
    "\n",
    "### building transformer section\n",
    "\n",
    "struct Transf{MHA<:Flux.MultiHeadAttention, D<:Flux.Dropout, LN<:Flux.LayerNorm, C<:Flux.Chain}\n",
    "    mha::MHA\n",
    "    att_dropout::D\n",
    "    att_norm::LN\n",
    "    mlp::C\n",
    "    mlp_norm::LN\n",
    "end\n",
    "\n",
    "function Transf(\n",
    "    embed_dim::Int, \n",
    "    hidden_dim::Int; \n",
    "    n_heads::Int, \n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    mha = Flux.MultiHeadAttention((embed_dim, embed_dim, embed_dim) => (embed_dim, embed_dim) => embed_dim, \n",
    "                                    nheads=n_heads, \n",
    "                                    dropout_prob=dropout_prob\n",
    "                                    )\n",
    "\n",
    "    att_dropout = Flux.Dropout(dropout_prob)\n",
    "    \n",
    "    att_norm = Flux.LayerNorm(embed_dim)\n",
    "    \n",
    "    mlp = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => hidden_dim, gelu),\n",
    "        Flux.Dropout(dropout_prob),\n",
    "        Flux.Dense(hidden_dim => embed_dim),\n",
    "        Flux.Dropout(dropout_prob)\n",
    "        )\n",
    "    mlp_norm = Flux.LayerNorm(embed_dim)\n",
    "\n",
    "    return Transf(mha, att_dropout, att_norm, mlp, mlp_norm)\n",
    "end\n",
    "\n",
    "Flux.@functor Transf\n",
    "\n",
    "function (tf::Transf)(input::Float32Matrix3DType) # input shape: embed_dim × seq_len × batch_size\n",
    "    normed = tf.att_norm(input)\n",
    "    atted, _ = tf.mha(normed, normed, normed) # outputs a tuple (a, b)\n",
    "    att_dropped = tf.att_dropout(atted)\n",
    "    residualed = input + att_dropped\n",
    "    res_normed = tf.mlp_norm(residualed)\n",
    "\n",
    "    embed_dim, seq_len, batch_size = size(res_normed)\n",
    "    reshaped = reshape(res_normed, embed_dim, seq_len * batch_size) # dense layers expect 2D inputs\n",
    "    mlp_out = tf.mlp(reshaped)\n",
    "    mlp_out_reshaped = reshape(mlp_out, embed_dim, seq_len, batch_size)\n",
    "    \n",
    "    tf_output = residualed + mlp_out_reshaped\n",
    "    return tf_output\n",
    "end\n",
    "\n",
    "struct Model{E<:Flux.Embedding, P<:PosEnc, D<:Flux.Dropout, T<:Flux.Chain, C<:Flux.Chain}\n",
    "    embedding::E\n",
    "    pos_encoder::P\n",
    "    pos_dropout::D\n",
    "    transformer::T\n",
    "    classifier::C\n",
    "end\n",
    "\n",
    "function Model(;\n",
    "    input_size::Int,\n",
    "    embed_dim::Int,\n",
    "    n_layers::Int,\n",
    "    n_classes::Int,\n",
    "    n_heads::Int,\n",
    "    hidden_dim::Int,\n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    embedding = Flux.Embedding(input_size => embed_dim)\n",
    "\n",
    "    pos_encoder = PosEnc(embed_dim, input_size)\n",
    "\n",
    "    pos_dropout = Flux.Dropout(dropout_prob)\n",
    "\n",
    "    transformer = Flux.Chain(\n",
    "    (Transf(embed_dim, hidden_dim; n_heads, dropout_prob) for _ in 1:n_layers)...\n",
    "    )\n",
    "\n",
    "    classifier = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => embed_dim, gelu),\n",
    "        Flux.LayerNorm(embed_dim),\n",
    "        Flux.Dense(embed_dim => n_classes)\n",
    "        )\n",
    "\n",
    "    return Model(embedding, pos_encoder, pos_dropout, transformer, classifier)\n",
    "end\n",
    "\n",
    "Flux.@functor Model\n",
    "\n",
    "function (model::Model)(input::T) where {T<:IntMatrix2DType} \n",
    "    # there is an issue here - where type is Any from the Flux portion\n",
    "    # now - if Flux is causing issues, go into source code and redefine as above:\n",
    "    # (m::Embedding)(x::T) where {T<:AbstractArray} = reshape(m(vec(x)), :, size(x)...), copied from Flux source code\n",
    "    # AND\n",
    "    # input::T where T<:type, allows it to be distinguished as a subtype of the input type\n",
    "    # should theoretically be able to avoid Anys, and be type-stable!\n",
    "    embedded = model.embedding(input)\n",
    "    encoded = model.pos_encoder(embedded)\n",
    "    encoded_dropped = model.pos_dropout(encoded)\n",
    "    transformed = model.transformer(encoded_dropped)\n",
    "    pooled = dropdims(mean(transformed; dims=2), dims=2)\n",
    "    logits_output = model.classifier(pooled)\n",
    "    return logits_output\n",
    "    return embedded\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee13bed",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- need to \n",
    "    1. ~~re-run with fixed typing~~ faster.jl running on kraken gpu 1\n",
    "\n",
    "    2. ~~figure out why test is better than train for loss/accuracy (potentially change in indef_run code or run mask_transf code for x epochs if test is only better in indef_run and not mask_transf)~~ ~~indef_run.jl code changed, running new on smaug gpu 2 ONCE OLD_INDEF_RUN.JL gets to 40!!! --> running new one now! what was the diff bruh~~ doen + clarified fix, see indef_masked_rankings 08-04 vs. 08-05. issue was the withgradient (AGAIN!!!)\n",
    "\n",
    "    3. ~~fix param logging for exp_transf + mask_transf (predstrues.csv, params.txt)~~\n",
    "\n",
    "    4. ~~fix progressbar for indef_run.jl~~ removed progress bar lol\n",
    "\n",
    "    5. ~~fix scatter plot for mask_transf comparison~~ ~~mask_transf_err.jl running on kraken gpu 0~~ done, see masked_rankings/2025-08-05\n",
    "\n",
    "    6. ~~x-bin for exp_transf comparison~~ ~~exp_transf.jl running on smaug gpu 3~~ done, see masked_expression/2025-08-05\n",
    "\n",
    "    7. reorganize exp, mask, indef, faster for tomorrow\n",
    "\n",
    "    8. put fxns/structs into separate src files! more organized.\n",
    "\n",
    "- 08-04 run is original test/loss calculations from creating 720ep graph\n",
    "- 08-05 run is updated calculations from mask_transf code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af32f5",
   "metadata": {},
   "source": [
    "aug 6, 2025 - recap\n",
    "="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ad1644",
   "metadata": {},
   "source": [
    "asap:\n",
    "- ~~exp_transf.jl running on kraken 0 (10ep x-bin)~~ done\n",
    "- ~~mask_transf_err.jl running on smaug 3 (10ep heatmap)~~ done\n",
    "\n",
    "still pending:\n",
    "- faster.jl running on kraken gpu 1 --> old code: 668774 ms, new code:\n",
    "    - terminated - need to fix code\n",
    "- reorganize exp, mask, indef, faster\n",
    "- put fxns/structs into separate src files! more organized.\n",
    "- ~~why not: exp transf run on untrt - kraken 0~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f18c7db",
   "metadata": {},
   "source": [
    "aug 12, 2025 - predicting the average, ensuring no repeats, fixing plots\n",
    "="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5098b",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- ~~redo heatmap/x-bin into boxplots or hex-bin?~~\n",
    "    ~~- exp on kraken 0, rank on smaug 0~~\n",
    "    - longer rank run on smaug 0\n",
    "- ~~see if model is just prev the avg rather than acc learning (raw exp)~~\n",
    "    - in exp code, running 100ep on kraken 0 for comparison\n",
    "- ~~see if possible to ensure model has no repeats (via permutations, inductive bias, pointer networks)~~\n",
    "    - trying on smaug 1 (need to clean up and understand tho)\n",
    "- put fxns/structs into separate src files! more organized.\n",
    "- make new diagrams! (look into CLE token)\n",
    "- do test iwthout pretrain to see if masking even helps\n",
    "- faster.jl ; compare memory usage still high!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fa216",
   "metadata": {},
   "source": [
    "aug 14, 2025 - for ensuring no repeats\n",
    "="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b80874",
   "metadata": {},
   "source": [
    "a pointer network is designed to select its output from the elements that are **present in the input sequence**. however, the correct answer (the masked number) is the one element that is explicitly absent from the input.\n",
    "\n",
    "https://kierszbaumsamuel.medium.com/pointer-networks-what-are-they-c3cb68fae076#:~:text=Notice%20how%20they%20are%20placed,-%20output%20dictionary%3A\n",
    "\n",
    "https://arxiv.org/abs/1506.03134 - pointer networks\n",
    "\n",
    "https://arxiv.org/abs/2006.06380 - pointer graph networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf26e77",
   "metadata": {},
   "source": [
    "alternatively:\n",
    "\n",
    "can have two sets of inputs:\n",
    "- context: masked sequence [1, 2, ..., MASK, 79, ...].\n",
    "- candidate: complete, unmasked set of all possible tokens [1, 2, ..., 978].\n",
    "\n",
    "where:\n",
    "- encoder processes the context input to understand what's missing.\n",
    "- decoder or attention mechanism then uses this context to point to the correct token within the candidate input.\n",
    "\n",
    "thus, the model isn't pointing to the sequence it was given but to a complete \"dictionary\" of possibilities, using the masked sequence to figure out which item in the dictionary is the right one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee8c15",
   "metadata": {},
   "source": [
    "is this realistically better?\n",
    "\n",
    "standard transf:\n",
    "- model must produce a single vector for the [MASK] token that, after going through one final linear layer, can be classified as 78\n",
    "- vector has to implicitly encode the identity \"78\"\n",
    "- model learns a complex, abstract function to map from context to an identity\n",
    "\n",
    "pointer:\n",
    "- model must produce a query vector for the [MASK] token\n",
    "- query's job is to be more similar to the vector for 78 in the candidate set than to any other number's vector\n",
    "- forces the model to learn a shared, consistent embedding space for all numbers\n",
    "- representation for 78 must be similar whether it's in the input or in the candidate list\n",
    "- encourages the model to learn the concept of \"78-ness\" in a way that is directly comparable to the concepts of \"77-ness\" and \"79-ness.\"\n",
    "\n",
    "THUS, leads to a more structured and relational embedding space..?\n",
    "\n",
    "*useful for things like travelling salesman problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bec63f",
   "metadata": {},
   "source": [
    "similar to: https://arxiv.org/abs/2005.11401\n",
    "\n",
    "RAG models improve language models by first using the input to retrieve relevant documents from a vast database (like Wikipedia). The model then uses both the original input and the retrieved documents to generate a final answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26396b47",
   "metadata": {},
   "source": [
    "aug 18, 2025 - currently\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf90c59",
   "metadata": {},
   "source": [
    "running rn:\n",
    "- smaug 0: ranking long run (300ep) with updated metrics plotting\n",
    "\n",
    "- ~~smaug 1: exp long run (300ep) wiht updated metrics plotting (and checking if just pred avg)~~\n",
    "    - done, not just pred avg; did well! (2025-08-18_16-03)\n",
    "\n",
    "working on rn:\n",
    "- ~~seeing if model is just predicting the average~~\n",
    "    - i think done, seems that model is doing better than average (2025-08-18_16-03)\n",
    "- investigating pointer networks and/or permutations, inductive bias\n",
    "- lux.jl to decrease mem allocs?\n",
    "- do test w/ and w/o pretrain to see if masking even helps\n",
    "    - should save weights of model as well (or model itself somehow for downstream applications)\n",
    "- reorganize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07f73c",
   "metadata": {},
   "source": [
    "aug 19, 2025 - no repeats (more options)\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe47c5b",
   "metadata": {},
   "source": [
    "some ideas:\n",
    "- inductive bias on the logits before softmax\n",
    "- constrained beam search (decoder-side control)\n",
    "- energy-based; adding a penalty for choosing a forbidden value\n",
    "    - allows for soft penalties .. not sure if this is good or bad\n",
    "- ~~output as a set (Set Transformers, DeepSets, or Determinantal Point Processes (DPPs))~~\n",
    "    - implies order doesn't matter?\n",
    "- ~~ILP/SAT decoding??~~\n",
    "    - not scalable\n",
    "- symbolic rule-based filter or constraint-satisfaction layer\n",
    "    - like logic tensor networks\n",
    "- copy/generate models\n",
    "    - also soft penalties; can forbid copy mode (in input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d68e6a",
   "metadata": {},
   "source": [
    "ones w/ hard penalties:\n",
    "- inductive bias\n",
    "    - isn't this what i am already doing via masking?\n",
    "    - no; rn i have data-level masking. inductive bias is prediction-level masking!\n",
    "- constrained beam search\n",
    "    - does not scale well too 100k samples\n",
    "- neural/symbolic hybrid\n",
    "    - 2 stage process - not sure how to backprop thru symbolic rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb0f11c",
   "metadata": {},
   "source": [
    "inductive bias:\n",
    "- before applying softmax (in loss function, after compute logits), you should set the logits for:\n",
    "    - any gene IDs already present in the input for that sample (so they can’t be predicted), and\n",
    "    - any gene IDs already predicted in previous decoding steps (if doing sequential prediction).\n",
    "- to -Inf (or a very negative number).\n",
    "- this ensures that the probability of those disallowed gene IDs is exactly 0.\n",
    "- rn code allows the model to assign probability mass to any of the n_classes outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2fada2",
   "metadata": {},
   "source": [
    "constrained beam search:\n",
    "- ~~greedy~~\n",
    "    - At each position t (row), you look at the logits for that position and immediately pick the single best legal option (argmax after masking forbidden genes).\n",
    "    - If enforce_unique=true, you also remove that choice from future positions.\n",
    "    - The decision is locally optimal (best at that timestep given constraints).\n",
    "- compact\n",
    "    - Instead of committing immediately, you keep the top-k partial hypotheses (“beams”) as you decode.\n",
    "    - At each timestep, every beam expands to multiple candidates (respecting constraints), then you prune back down to the top-k by cumulative score.\n",
    "    - After the last position, you return the best sequence.\n",
    "    - The decision is globally optimized across the sequence within beam budget."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38d669",
   "metadata": {},
   "source": [
    "neural/symbolic hybrid\n",
    "- split prediction into two modules:\n",
    "    - neural module: proposes a ranked list of candidate genes\n",
    "    - symbolic module: enforces your biological or structural rules\n",
    "- this is like a two-stage pipeline: model suggests --> rules finalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f54fa",
   "metadata": {},
   "source": [
    "SO FINAL IDEAS TO IMPLEMENT:\n",
    "- compact beam search\n",
    "    - 2017: https://arxiv.org/pdf/1704.07138 (grid beam search)\n",
    "    - 2018: https://arxiv.org/pdf/1804.06609 (dynamic beam allocation)\n",
    "    - explanation: https://huggingface.co/blog/constrained-beam-search\n",
    "- neural/symbolic constraints\n",
    "    - 2021: https://arxiv.org/pdf/2103.17232 (first introduction)\n",
    "    - 2024: https://arxiv.org/pdf/2410.20957 (logical constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe110bb",
   "metadata": {},
   "source": [
    "aug 20, 2025 - post-seb meeting\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b19ff",
   "metadata": {},
   "source": [
    "- comparison plots\n",
    "    - ~~waiting on smaug 0~~ done\n",
    "    - ~~- show distribution of true values in histogram; has very little values below 5 so we can ignore that~~ done; explain the distribution correlation with the accuracy as well!\n",
    "    - ~~- plot both boxplot and hist on top of one another with same axes~~\n",
    "    - ~~- see if possible to top whisker at 0.9 quartile and bottom whisker at 0.1 quartile; thus 10% of pts at upper and lower whisker rather than extrapolating data~~ done; not much different but easier to explain plot\n",
    "    - ~~- currently, what is used is IQR of 1.5 for whisker length~~\n",
    "    - ~~- try rangebars or @recipe macro on CairoMakie~~\n",
    "- just predicting avg expression?\n",
    "    - can also show this in the boxplot by adding an X where the average is (maybe no need)\n",
    "    - ~~- also, double check if we are comparing the hexbin of true values vs. average of predicted values (correct) or average of true values (incorrect)~~ correct!\n",
    "- improving ranked model\n",
    "    - ensure parameters are on the same scale generally; such that the exp and rank models have the same capacity\n",
    "    - since the rank model has less info in the input, it doing just as good as the exp model is sufficient\n",
    "    - for eval:\n",
    "        - can do a downstream task, OR\n",
    "        - we can compare actual values of expression model output to rank model output without needing significant inductive biases applied or downstream tasks\n",
    "        - need to look into how to have input = rank, output = expression value (for ranked model)\n",
    "        - ex. quantile normalization (w/ same distribution?)\n",
    "- slurm connect\n",
    "- review how exp model works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c27760",
   "metadata": {},
   "source": [
    "sep 3, 2025 - check-in\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ed6b8",
   "metadata": {},
   "source": [
    "plotting\n",
    "- new_boxplot.png compared to old boxplot in aug22 ppt\n",
    "- box_hex.png\n",
    "\n",
    "improving ranked model\n",
    "- opt 1: input = rank, output = expression val\n",
    "    - A. concatenate/add ranked embeddings + raw expression embeddings\n",
    "        - but this means that the rank model isn't 100% a rank model\n",
    "    - B. change to regression task w/ only model structure, fwd pass, and loss\n",
    "        - structure: classifier --> Flux.Chain(embed_dim => hidden_dim => 1)\n",
    "            - 1 for each value masked; loss is calculated on each masked value individually\n",
    "        - fwd pass: somewhat same, or we can use pooling?\n",
    "            - involves taking the mean across the sequence length\n",
    "        - loss: logit cross-entropy --> MSE?\n",
    "        - similar to what is already done in exp_transf.jl, just with diff input?\n",
    "    - C. quantile normalization\n",
    "        - replaces the integer rank with float of average expression for each gene\n",
    "        - then can convert that into a rank but maintain the previous raw values for y-labels?\n",
    "        - then predict singular value (Flux.Chain => 1) for expression prediction?\n",
    "        - does this generalize too much tho?\n",
    "- opt 2: constraint stuff\n",
    "    - ex. constrained beam search, neural/symbolic hybrid\n",
    "\n",
    "misc\n",
    "- downstream task?\n",
    "- slurm connect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d36de83",
   "metadata": {},
   "source": [
    "sep 4, 2025 - post- seb meeting\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e27511",
   "metadata": {},
   "source": [
    "plotting\n",
    "- get correlation of avg hexbin for comparison\n",
    "- update entropu graphs for comparison\n",
    "    - ex. have accuracy/entropy vs. rank and vs. expression\n",
    "        - look at density of probabilty for the expression one (but also seb said maybe no for this)\n",
    "        - re-running error vs rank for trt dataset on kraken 0\n",
    "    - see if the movement in position is correlated iwth cell type as well!\n",
    "    - look into more explanation stuff like this^^^\n",
    "\n",
    "dataset\n",
    "- do LINCS treated and untreated dataset\n",
    "    - for exmaple, if we have larger dataset (by x10) then do # epochs / 10 (300 ep on untrt, 30 ep on trt/untrt); thus we have the same # of gradient steps\n",
    "    - try to run a small subsample first for speed and testing\n",
    "    - rank currently running on smaug 0, exp currenty running on smaug 1 (09-07)\n",
    "\n",
    "task\n",
    "- predict cell line then predict average expression of that cell line\n",
    "\n",
    "models\n",
    "- use FNN/MLP for expression profile with same number of parameters as ranking transformer\n",
    "    - as the initial comparison\n",
    "\n",
    "evaluation\n",
    "- find more differences? but masking task might be sufficient\n",
    "- look into autoencoders which ask to recover values from 0 (ex. denoising autoencoder); is this similar to masking alr tho?\n",
    "\n",
    "validating it is acc learning\n",
    "- could remove the mean expression for each gene before input; thus it is definitely not just learning the average; but instead if avg is 0 then it is definitley learning properly\n",
    "    - this also means i'd need to change the MASK from -1 to something else (ex. a collection of 0s and 1s..?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20300b8",
   "metadata": {},
   "source": [
    "sep 9, 2025 - prep ppt for meeting fri\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e48bb3",
   "metadata": {},
   "source": [
    "for ppt:\n",
    "- introduction, model, how masking works,etc\n",
    "- rank vs. exp comaprison\n",
    "    - plot of train/test loss\n",
    "    - plot of boxplots of pred vs true\n",
    "        - with a histogram for the exp val\n",
    "        - no histogram needed for the rank; it is uniform distr\n",
    "    - also error only for rank (should there be error for exp as well???)\n",
    "    - if smaug done in time, show big params vs smol params runs (30ep vs 5ep)\n",
    "- reasoning results\n",
    "    - plot of entropies in dataset\n",
    "    - plot of prediction error by value\n",
    "    - plot of avg hexbin???\n",
    "- new task!\n",
    "- new model..?\n",
    "***\n",
    "\n",
    "plotting\n",
    "- updated entropy graphs to include trt data\n",
    "    - rank: done\n",
    "    - exp: done\n",
    "        - using bins of size 0.01 and 0.1 for discretization\n",
    "- to compare to entropy graphs, re-run on trt data for error vs:\n",
    "    - rank: **running on kraken 0, 5ep**\n",
    "    - exp: **running on kraken 1, 5ep**\n",
    "- plot cell type against error\n",
    "    - cell type vs. error: ?\n",
    "    - cell type vs. entropy: done\n",
    "- get explanations for above\n",
    "- get correlation of avg hexbin for comparison (not urgent)\n",
    "\n",
    "dataset --> changed from untrt only to trt and untrt\n",
    "- long run with big params\n",
    "    - rank: **running on smaug 0, 30ep**\n",
    "    - exp: **running on smaug 1, 30ep**\n",
    "\n",
    "task\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdbc4cf",
   "metadata": {},
   "source": [
    "sep 12, 2025 - parameter checking, organizing todo\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1d9d3",
   "metadata": {},
   "source": [
    "1. comparing input results\n",
    "- be able to explain projection on raw vs. embedding on rank\n",
    "- check: does exp model have only 128 params per input vector via Wx+b (128\\*1) while the rank model has 128\\*978 params where each value gets projected the weight dimension?\n",
    "    - this would result in less complexity for the embedding model, which means that the embedding model has a disadvantage\n",
    "    - **EXP MODEL # PARAMS: 649,649**\n",
    "        - via total_params = sum(length, Flux.params(model))\n",
    "        - embed_dim = 128, hidden_dim = 236, n_heads = 2, n_layers = 4\n",
    "    - **RANK MODEL # PARAMS: 921,426**\n",
    "        - embed_dim = 128, hidden_dim = 256, n_heads = 2, n_layers = 4\n",
    "        - params length = 56\n",
    "        - (128, 979)(128, 979)(128, 128)(128, 128)(128, 128)(128, 128)(128,)(128,)(256, 128)(256,)(128, 256)(128,)(128,)(128,)(128, 128)(128, 128)(128, 128)(128, 128)(128,)(128,)(256, 128)(256,)(128, 256)(128,)(128,)(128,)(128, 128)(128, 128)(128, 128)(128, 128)(128,)(128,)(256, 128)(256,)(128, 256)(128,)(128,)(128,)(128, 128)(128, 128)(128, 128)(128, 128)(128,)(128,)(256, 128)(256,)(128, 256)(128,)(128,)(128,)(128, 128)(128,)(128,)(128,)(978, 128)(978,)\n",
    "\n",
    "2. entropy vs. error graphs **--> APPLY TO THE 30EP RUN**\n",
    "- rank:\n",
    "    - take mean/avg error per rank instead for better visualization\n",
    "- expression:\n",
    "    - double check the p(x) distribution of the bins; are they fitting in properly or are some bins have 0?\n",
    "    - do std. dev of expression (or potentially interquartile distance) rather than entropy - this has better representation of variation\n",
    "3. next steps\n",
    "- MLP as an autoencoder; train the beginning then prediction head at the end is different - to sub in for downstream task\n",
    "    - aka pretraining an MLP?\n",
    "    - see denoising autoencoder\n",
    "- stacked ass autoencoder vs. transformer with same number of parameters (same degrees of freedom)\n",
    "    - if autoencoder better then yay! if transformer better then boo.\n",
    "- see leo's bottleneck stuff\n",
    "- look at benchmarks\n",
    "    - aka lea's; predicting gene expression from a different cell line and same perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a726174",
   "metadata": {},
   "source": [
    "currently running: 30ep exp run on smaug 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac4561",
   "metadata": {},
   "source": [
    "masked model\n",
    "-\n",
    "julia> model.classifier\n",
    "Chain(\n",
    "  Dense(128 => 128, gelu_tanh),         # 16_512 parameters\n",
    "  LayerNorm(128),                       # 256 parameters\n",
    "  Dense(128 => 978),                    # 126_162 parameters\n",
    ")                   # Total: 6 arrays, 142_930 parameters, 992 bytes.\n",
    "\n",
    "julia> model.embedding\n",
    "Embedding(979 => 128)  # 125_312 parameters\n",
    "\n",
    "julia> model.pos_encoder\n",
    "PosEnc(Float32[0.84147096 0.9092974 … -0.8218694 -0.92342377; 0.5403023 -0.41614684 … -0.56967604 0.38378194; … ; 0.0001154782 0.0002309564 … 0.11269774 0.11281249; 1.0 1.0 … 0.99362934 0.9936163])\n",
    "\n",
    "julia> model.transformer\n",
    "Chain(\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 256, gelu_tanh),     # 33_024 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(256 => 128),                # 32_896 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 256, gelu_tanh),     # 33_024 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(256 => 128),                # 32_896 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 256, gelu_tanh),     # 33_024 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(256 => 128),                # 32_896 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 256, gelu_tanh),     # 33_024 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(256 => 128),                # 32_896 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    ")                   # Total: 48 arrays, 527_872 parameters, 8.328 KiB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29172b1",
   "metadata": {},
   "source": [
    "exp model\n",
    "-\n",
    "julia> model.classifier\n",
    "Chain(\n",
    "  Dense(128 => 128, gelu_tanh),         # 16_512 parameters\n",
    "  LayerNorm(128),                       # 256 parameters\n",
    "  Dense(128 => 1, softplus),            # 129 parameters\n",
    ")                   # Total: 6 arrays, 16_897 parameters, 992 bytes.\n",
    "\n",
    "julia> model.projection\n",
    "Dense(1 => 128)     # 256 parameters\n",
    "\n",
    "julia> model.pos_encoder\n",
    "PosEnc(Float32[0.84147096 0.9092974 … 0.035307925 -0.8218694; 0.5403023 -0.41614684 … -0.9993765 -0.56967604; … ; 0.0001154782 0.0002309564 … 0.112583004 0.11269774; 1.0 1.0 … 0.99364233 0.99362934])\n",
    "\n",
    "julia> model.transformer\n",
    "Chain(\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 236, gelu_tanh),     # 30_444 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(236 => 128),                # 30_336 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 236, gelu_tanh),     # 30_444 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(236 => 128),                # 30_336 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 236, gelu_tanh),     # 30_444 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(236 => 128),                # 30_336 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 236, gelu_tanh),     # 30_444 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(236 => 128),                # 30_336 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    ")                   # Total: 48 arrays, 507_312 parameters, 8.328 KiB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65547872",
   "metadata": {},
   "source": [
    "rank model:\n",
    "- \n",
    "- input:\n",
    "    - Embedding(979 => 128) layer\n",
    "    - creates a lookup table to convert each of 979 unique input tokens into a 128-dimensional vector\n",
    "    - 979×128=125,312 parameters\n",
    "- output:\n",
    "    - Dense(128 => 978) layer\n",
    "    -  takes the final 128-dimensional representation and projects it into 978 output values, corresponding to the probability of each token in the vocabulary\n",
    "    - 128×978+978=126,162 parameters\n",
    "\n",
    "exp model:\n",
    "- input:\n",
    "    - Dense(1 => 128) layer\n",
    "    - takes a single number and projects it into a 128-dimensional vector\n",
    "    - 1×128+128=256 parameters.\n",
    "- output: \n",
    "    - Dense(128 => 1, softplus)\n",
    "    - outputs a single number\n",
    "    - 128×1+1=129 parameters\n",
    "\n",
    "difference: ~250k parameters\n",
    "\n",
    "~~**ADDITIONALLY:~~ **--> FIXED**\n",
    "- masked: trasnf netwrok expands the dimension from 128 to 256 and then back to 128 (128 => 256 => 128)\n",
    "- exp: transf netwrok expands the dimension from 128 to 236 and then back to 128 (128 => 236 => 128)\n",
    "\n",
    "difference: ~20k parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53605f55",
   "metadata": {},
   "source": [
    "sep 15, 2025 - autoencoder research\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907d4827",
   "metadata": {},
   "source": [
    "https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf\n",
    "- stacked denoising autoencoders --> similar to masked pretrain objective\n",
    "    - difference:\n",
    "        - DAE = layer-wise pretrain; T = end-to-end pretrain\n",
    "        - DAE = local ft detection (neural net); T = global/bidirectional context awareness (via posenc too) (attention)\n",
    "    - similarity:\n",
    "        - adding noise = setting inputs to 0 = masking\n",
    "            - so one of the main things of the DAE is that it uses diff \"corruption\" methods; ex. Gaussian noise, masking (set to 0), salt/pepper noise (set to max/min val, uysualy either 0 or 1) \n",
    "            - reconstructing input = predicting identity of whatever was masked/hidden\n",
    "- for my project:\n",
    "    - essentially can use this as the \"MLP\" (try with/without the stacking)\n",
    "    - different reconstruction tasks other than masking; ex. Gaussian noise, salt/pepper noise?\n",
    "    - BUT should i be doing 1. exp DAE, 2. rank DAE, 3. exp transf, 4. rank transf?\n",
    "        - for comparison against both input aspects and architecture aspects?\n",
    "        - what else needs to be done in terms of supporting the idea that the baselines can perform just as well? (referencing https://www.nature.com/articles/s41592-025-02772-6#Sec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde7f969",
   "metadata": {},
   "source": [
    "reasoning:\n",
    "- https://arxiv.org/abs/2502.19718 (2025)\n",
    "    - information theory perspective on masked autoencoders\n",
    "\n",
    "**look into:**\n",
    "- can i use a DAE or MAE to compare against an MLM?\n",
    "    - shouldn't it be bidirectional like a transformer? or is it already?\n",
    "    - DAE: see above\n",
    "    - MAE: https://arxiv.org/pdf/1502.03509\n",
    "- what about contractive autoencoders: https://icml.cc/2011/papers/455_icmlpaper.pdf\n",
    "\n",
    "**some new stuff:**\n",
    "- https://www.nature.com/articles/s41598-025-96215-z\n",
    "- https://arxiv.org/abs/2505.22914"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b000ab",
   "metadata": {},
   "source": [
    "for tmo:\n",
    "- leo's bottleneck stuff\n",
    "- lea's benchmarking?\n",
    "- other benchmarking?\n",
    "- prep for meeting; other reasonings why to do or not to do DAE/MAE (specifically the quad-comparison as mentioned td above)\n",
    "- difference between AE and MLP or FNN? or same thing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d47e1b",
   "metadata": {},
   "source": [
    "sep 16, 2025 - organizing objectives\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab62e8",
   "metadata": {},
   "source": [
    "for input comparison:\n",
    "- i'm thinking of having exp vs. rank on a transformer and exp vs. rank on a FNN...?\n",
    "- currently the only difference i have between the inputs is that the degree of variation per token learned is a lot greater in the ranked input (measured in entropy) compared to the raw expression value input (measured in variance)\n",
    "\n",
    "for architecture comparison:\n",
    "- currently there are ~250k more parameters in the masked model (649,649 vs. 921,426)\n",
    "    - this is due to:\n",
    "        - rank model:\n",
    "            - input: Embedding(979 => 128) layer (979×128=125,312 params)\n",
    "            - output: Dense(128 => 978) layer (128×978+978=126,162 params) - +978 is due to # biases\n",
    "        - exp model:\n",
    "            - input: Dense(1 => 128) layer (1×128+128=256 params) - +128 is due to # biases\n",
    "            - output: Dense(128 => 1, softplus) (128×1+1=129 params) - +1 is due to # biases\n",
    "- mainly: FNN against MLM:\n",
    "    - should the FNN be 1 layer, DAE, or MAE? (or maybe try all 3?)\n",
    "\n",
    "update on plotting:\n",
    "- entropy vs. error graphs **--> APPLY TO THE 30EP RUN**\n",
    "    - rank:\n",
    "        - take mean/avg error per rank instead for better visualization\n",
    "    - expression:\n",
    "        - double check the p(x) distribution of the bins; are they fitting in properly or are some bins have 0?\n",
    "        - do std. dev of expression (or potentially interquartile distance) rather than entropy - this has better representation of variation\n",
    "- get correlation of avg hexbin for comparison in supplementary\n",
    "\n",
    "it was mentioned last wk:\n",
    "- look at benchmarks\n",
    "    - leo's; some bottleneck stuff?\n",
    "    - lea's; predicting gene expression from a different cell line and same perturbation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6057b6a1",
   "metadata": {},
   "source": [
    "for reference:\n",
    "- MLP\n",
    "    - supervised learning (vs. AE is unsupervised)\n",
    "    - input layer + hidden layer(s) + output layer; loss is calcualted by comparing output to true label\n",
    "- DAE\n",
    "    - corrupts input data and decoder reconstructs the original; loss is calculated between reconstructed form z and original input x\n",
    "    - https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf (stacked DAE)\n",
    "    - https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf (robust ft w/ DAEs)\n",
    "- MAE\n",
    "    - in images: randomly mask 75% of an image, encoding only the visible patches and decoder reconstructs the original\n",
    "    - outside of images:\n",
    "        - https://arxiv.org/abs/2309.13793 (ReMasker)\n",
    "            - original input has missing values; additional inputs are masked for AE to train on - then applied to original missing values\n",
    "        - https://arxiv.org/abs/2412.19152 (PMAE)***\n",
    "            - masks original input w/ a probability inversely related to a column's observation rate\n",
    "            - ensures that the model must learn more from rarer/less frequently observed features\n",
    "- CAE\n",
    "    - AE w/ small penalty to loss fxn to reduce sensitivity to small/local variations in the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d6635d",
   "metadata": {},
   "source": [
    "todo: \n",
    "- ~~does MAE exist outside of images/comp vision?~~ yes; see above\n",
    "- ~~wtf does MAE for distribution estimation do?~~ learns data distribution from reconstructing masked input\n",
    "- ~~finish why params are different~~\n",
    "- ~~redo error per rank graph~~ @ /home/golem/scratch/chans/lincs/plots/trt_and_untrt/masked_rankings/2025-09-11_08-26\n",
    "- ~~redo entropy (std dev instead) per exp graph (on untrt since trt still running)~~\n",
    "    - check bin distribution of expression error (on untrt since trt still running)\n",
    "- do avg exp of genes then sort from hgihest to lowest for comparison against rank error graph\n",
    "- look more into reasoning why input diff = terrible ranked input performance\n",
    "\n",
    "later:\n",
    "- avg hexbin correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2cd1b0",
   "metadata": {},
   "source": [
    "sep 17, 2025\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec406e",
   "metadata": {},
   "source": [
    "INPUT COMPARISON:\n",
    "- i'm thinking of having exp vs. rank on a transformer and exp vs. rank on a FNN...?\n",
    "- currently the only difference i have between the inputs is that the degree of variation per token learned is a lot greater in the ranked input (measured in entropy) compared to the raw expression value input (measured in variance)\n",
    "\n",
    "ARCHITECTURE COMPARISON:\n",
    "- currently there are ~250k more parameters in the masked model (649,649 vs. 921,426)\n",
    "    - this is due to:\n",
    "        - rank model:\n",
    "            - input: Embedding(979 => 128) layer (979×128=125,312 params)\n",
    "            - output: Dense(128 => 978) layer (128×978+978=126,162 params) - +978 is due to # biases\n",
    "        - exp model:\n",
    "            - input: Dense(1 => 128) layer (1×128+128=256 params) - +128 is due to # biases\n",
    "            - output: Dense(128 => 1, softplus) (128×1+1=129 params) - +1 is due to # biases\n",
    "- mainly: FNN against MLM:\n",
    "    - should the FNN be 1 layer, DAE, or MAE? (or maybe try all 3?)\n",
    "\n",
    "PLOTTING:\n",
    "- entropy vs. error graphs **--> APPLY TO THE 30EP RUN**\n",
    "    - rank: (trt-rankings)\n",
    "        - see rank_vs_avgerror (scatter vs line.png)\n",
    "    - expression: (infographs)\n",
    "        - see gene_exp_std_dev_trt.png (is this ok or is IQ distance a better representation?)\n",
    "    - sorted gene expression: (infographs)\n",
    "        - sorted_gene_mean_exp_trt.png for validation of sorting\n",
    "        - sorted_gene_std_dev_trt.png for comparison against rank_entropy_trt.png\n",
    "    - sorted exp error: (unrtrt-exp)\n",
    "        - gene_vs_meanerror.png vs. sorted_gene_vs_meanerror.png\n",
    "        - sorted_gene_vs_meanerror.png vs. rank_vs_avgerror_scatter.png\n",
    "\n",
    "MENTIONED:\n",
    "- look at benchmarks\n",
    "    - leo's; some bottleneck stuff?\n",
    "    - lea's; predicting gene expression from a different cell line and same perturbation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed1eccc",
   "metadata": {},
   "source": [
    "FOR REFERENCE:\n",
    "- MLP\n",
    "    - supervised learning (vs. AE is unsupervised)\n",
    "    - input layer + hidden layer(s) + output layer; loss is calcualted by comparing output to true label\n",
    "- DAE\n",
    "    - corrupts input data and decoder reconstructs the original; loss is calculated between reconstructed form z and original input x\n",
    "    - https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf (stacked DAE)\n",
    "    - https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf (robust ft w/ DAEs)\n",
    "- MAE\n",
    "    - in images: randomly mask 75% of an image, encoding only the visible patches and decoder reconstructs the original\n",
    "    - outside of images:\n",
    "        - https://arxiv.org/abs/2309.13793 (ReMasker)\n",
    "            - original input has missing values; additional inputs are masked for AE to train on - then applied to original missing values\n",
    "        - https://arxiv.org/abs/2412.19152 (PMAE)***\n",
    "            - masks original input w/ a probability inversely related to a column's observation rate\n",
    "            - ensures that the model must learn more from rarer/less frequently observed features\n",
    "            - more missing values = more mask (complete columns used for reconstruction)\n",
    "            - evaluated w/ coefficient of determination (numerical values) and accuracy (categorical values)\n",
    "            - argued that the MAE was better than the transformer version for local dependencies\n",
    "\n",
    "- CAE\n",
    "    - AE w/ small penalty to loss fxn to reduce sensitivity to small/local variations in the input\n",
    "\n",
    "aside:\n",
    "- https://arxiv.org/abs/2105.01601\n",
    "    - stacked MLP for computer vision that achieved competitive (around the same) scores compared to CNNs and vision transformers\n",
    "    - this is the model taht PMAE paper used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abab29b",
   "metadata": {},
   "source": [
    "sep 18, 2025 - todo\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298be776",
   "metadata": {},
   "source": [
    "INPUT COMPARISON:\n",
    "- complete reasoning results\n",
    "- complete error explanations\n",
    "- get graphs for exp 30ep (on smaug 0 rn)\n",
    "- begin run on rank 30ep afterwards\n",
    "\n",
    "ARCHITECTURE COMPARISON:\n",
    "- complete reasoning results\n",
    "- complete error explanations\n",
    "\n",
    "PLOTTING:\n",
    "- rank:\n",
    "- expression:\n",
    "        - double check the p(x) distribution of the bins; are they fitting in properly or are some bins have 0?\n",
    "- get correlation of avg hexbin for comparison in supplementary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1725fae",
   "metadata": {},
   "source": [
    "sep 20, 2025 - fixing 30ep runs, reorganize/planing for cp\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c3c7f1",
   "metadata": {},
   "source": [
    "INPUT COMPARISON:\n",
    "- complete reasoning results\n",
    "- complete error explanations\n",
    "- ~~get graphs for trt exp 30ep (30ep on smaug 1 rn thru sbatch)~~ done!\n",
    "- get graphs for trt rank 30ep (30ep on smaug 0 rn thru nohup)\n",
    "   - sbatch for rank doesn't work; some kind of OOM error?\n",
    "      - likely becasue the flag has to be --mem-per-gpu rather than --mem (smaug has 300gb total! or something)\n",
    "\n",
    "ARCHITECTURE COMPARISON:\n",
    "- build FNN-DAE for exp and rank\n",
    "   - running exp_nn --> issue with code; need to fix\n",
    "      - right now, it's kinda cooked. going to try to make it so that its reconstructing the original input rather than reconstructing the embedding space (output = 978 \\* batch rather than output = 64 \\* batch)\n",
    "      - should masking be done before compressing input into the embedding or after??\n",
    "      - things changed:\n",
    "         - removed output 0.0f0 when sum(mask) = 0 in the loss\n",
    "         - added mlp_head in Model + function\n",
    "      - or acc no need to reconstruct original.... becasue embebdding shoud already be a more robust represetnaton of the input data?\n",
    "      - should be fine now;\n",
    "         - just 1. increase mask ratio, 2. decrease LR, 3. normalize input?\n",
    "   - rank_nn still pending\n",
    "- complete reasoning results\n",
    "- complete error explanations\n",
    "\n",
    "PLOTTING:\n",
    "- rank:\n",
    "- expression:\n",
    "        - double check the p(x) distribution of the bins; are they fitting in properly or are some bins have 0?\n",
    "- get correlation of avg hexbin for comparison in supplementary\n",
    "\n",
    "GENERALLY:\n",
    "- create a doc of everything done, tests, etc. need to create some kind of story for talks later + poster presentations in oct\n",
    "- start prepping poster :0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff30eed",
   "metadata": {},
   "source": [
    "some additional things brought up last meeting:\n",
    "\n",
    "INPUT\n",
    "- exp:\n",
    "    - try w/ + w/o positional encoding\n",
    "    - ~~is posenc independent of gene exp?~~ YES.\n",
    "        - the positional encoding is calculated is deterministic;\n",
    "        - only uses the position and the embedding dimension index\n",
    "        - no knowledge of the actual gene expression values\n",
    "    - ~~or is posenc = gene exp embedding?~~ NO.\n",
    "        - posenc is added to gene exp embedding\n",
    "        - \"input .+ pe.pe_matrix[:,1:seq_len]\"\n",
    "- rank: \n",
    "    - is it possibel to pass (posenc + exp) --> add to rank?\n",
    "        - yes; if we were to make a hybrid version -- try later!!!********** via adding/concatenating rank embedding and expression projected into same dim as embedding\n",
    "    - ~~does rank need embeddings?~~\n",
    "        - if i were to feed raw rank integers, the model would assume rank 2 > rank 1 based on numerical distance between numbers (which is not true)\n",
    "        - since they are arbitrary ids, each rank should have a embedding vector that represents its identity and meaning\n",
    "    - ~~try embed_dim = 1; since rank input val = arbitrary #~~\n",
    "        - it isn't an arbitrary number! see above\n",
    "\n",
    "ARCH\n",
    "- tf:\n",
    "    - output is either dummy embed vec returned from pretrain OR add/concat them together from 2nd last layer\n",
    "- nn:\n",
    "    - is bottleneck layer needed?\n",
    "- is it possibel to get tf to be as good as nn? (if tf not as good)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
