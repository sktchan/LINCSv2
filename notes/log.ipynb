{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7cedc9",
   "metadata": {},
   "source": [
    "<< to keep track of changes made, prev. ideas, etc. >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f3a3fe",
   "metadata": {},
   "source": [
    "apr 8, 2025 - flux_transf.jl\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b69bd4",
   "metadata": {},
   "source": [
    "this is what was used for original 2D input into mhsa (which doesn't work, needs (k, q, v) input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function (tf::Transf)(input::Float32Matrix2DType) # input is features (978) x batch\n",
    "    sa_out = tf.mhsa(tf.norm_1(input)) # OG paper states norm after sa, but norm before sa is more common?\n",
    "    # x = input + tf.dropout(sa_out)\n",
    "    x = input + sa_out\n",
    "    mlp_out = tf.mlp(tf.norm_2(x))\n",
    "    # x = x + tf.dropout(mlp_out)\n",
    "    x = x + mlp_out\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c617cb3",
   "metadata": {},
   "source": [
    "apr 11, 2025 - flux_transf.jl\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc0b2b",
   "metadata": {},
   "source": [
    "using a for loop and replacing a matrix of -1 rather than making a copy is much faster! \n",
    "using the function below (commented parts) is a little easier to read, but the current impl. in the file is much more efficient.\n",
    "tmp: if we were to replace the -1s with not the gene names (Str) but with the ranking ints themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function sort_gene(expr)\n",
    "    # data_ranked = Matrix{Int}(undef, size(expr))\n",
    "    data_ranked = fill(-1, size(expr))\n",
    "    # gs = Symbol.(gene_symbols)\n",
    "    n, m = size(expr)\n",
    "    p = Vector{Int}(undef, n)\n",
    "    # tmp = sortperm(expr[:, 1])\n",
    "\n",
    "    for j in 1:m\n",
    "        e = view(expr, :, j)\n",
    "        sortperm!(p, e, rev=true)\n",
    "        # data_ranked[!, j] = gs[tmp]\n",
    "\n",
    "        for i in 1:n\n",
    "            data_ranked[i, j] = p[i]\n",
    "        end\n",
    "    end\n",
    "    return data_ranked\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec85ec5f",
   "metadata": {},
   "source": [
    "GENES AS TOKENS gene-gene interactions (sequence length as len(genes))\n",
    "- each gene is a position in your sequence\n",
    "- each token's embedding contains information about that gene's ranking across samples\n",
    "- the sequence length equals the number of genes you're considering\n",
    "SAMPLES AS TOKENS sample-sample interactions (sequence length as len(samples))\n",
    "- each sample is a position in your sequence\n",
    "- each token's embedding contains the gene ranking information for that sample\n",
    "- the sequence length equals the number of samples\n",
    "TECHNICAL CONSIDERATIONS\n",
    "transformers struggle with very long sequences, so if we have many more genes than samples, using samples as tokens may be more computationally feasible\n",
    "self-attention complexity grows quadratically with sequence length\n",
    "which dimension has more examples to learn from?\n",
    "\n",
    "good option: Flux.MultiHeadAttention((64, 64, 64) => (64, 64) => 64, nheads=1), can incr nheads later\n",
    "- q, k, v input dim should all be the same if data type is the same or we aren't doing encoder-decoder\n",
    "- middle dimensions should also be the same unless we want to reduce computational complexity in the middle\n",
    "- output can also be the same unless we want to do ft compression or expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8df65",
   "metadata": {},
   "source": [
    "may 1, 2025 - masked loss fxn - DONE\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec7019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_masked(model, x, y_masked)\n",
    "    logits = model(x)  # (n_classes, seq_len, batch_size)\n",
    "    logits = permutedims(logits, (2, 3, 1))  # seq_len × batch_size × n_classes (to match targets)\n",
    "    logits = reshape(logits, :, n_classes)   # (seq_len * batch_size) × n_classes\n",
    "\n",
    "    y_masked_flat = vec(y_masked) # flatten\n",
    "    # only keep where y_masked != -100\n",
    "    mask = y_masked_flat .!= -100\n",
    "    logits_masked = (logits[mask, :])'\n",
    "    targets_masked = y_masked_flat[mask]\n",
    "    y_oh = Flux.onehotbatch(targets_masked, 1:n_classes)\n",
    "\n",
    "    return Flux.logitcrossentropy(logits_masked, y_oh)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f723c",
   "metadata": {},
   "source": [
    "may 27, 2025 - training fxn with masked values for accuracy - DONE\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ffb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss(model, x, y)\n",
    "    logits = model(x)  # (n_classes, seq_len, batch_size)\n",
    "    logits_flat = reshape(logits, size(logits, 1), :) # (n_classes, seq_len*batch_size)\n",
    "    y_flat = vec(y) # (seq_len*batch_size) column vec\n",
    "\n",
    "    mask = y_flat .!= -100 # bit vec, where sum = n_masked\n",
    "    logits_masked = logits_flat[:, mask] # (n_classes, n_masked)\n",
    "    y_masked = y_flat[mask] # (n_masked) column vec\n",
    "\n",
    "    y_oh = Flux.onehotbatch(y_masked, 1:n_classes) # (n_classes, n_masked)\n",
    "    return Flux.logitcrossentropy(logits_masked, y_oh) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505e3725",
   "metadata": {},
   "source": [
    "could return logits_masked and y_masked as well, then do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_masked = Flux.onecold(logits_masked)\n",
    "preds_masked_cpu = preds_masked |> cpu\n",
    "preds_masked_cpu .== y_masked\n",
    "accuracy = sum(preds_masked_cpu .== y_masked) / length(y_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604ef7db",
   "metadata": {},
   "source": [
    "may 29, 2025 - sparse matrices\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in train loop, instead of y_gpu:\n",
    "y_batch_sparse = get_sparse_batch(y_train_masked, start_idx, end_idx)\n",
    "\n",
    "using SparseArrays\n",
    "\n",
    "function mask_input_sparse(X::Matrix{Int64}; mask_ratio=0.10)\n",
    "    X_masked = copy(X)\n",
    "    # Create sparse matrix for labels\n",
    "    I_indices = Int[]  # row indices\n",
    "    J_indices = Int[]  # column indices  \n",
    "    values = Int16[]   # actual gene indices\n",
    "    \n",
    "    for j in 1:size(X, 2)\n",
    "        num_masked = ceil(Int, size(X, 1) * mask_ratio)\n",
    "        mask_positions = randperm(size(X, 1))[1:num_masked]\n",
    "        \n",
    "        for pos in mask_positions\n",
    "            push!(I_indices, pos)\n",
    "            push!(J_indices, j)\n",
    "            push!(values, X[pos, j])  # original gene index\n",
    "            \n",
    "            X_masked[pos, j] = MASK_ID\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    y_sparse = sparse(I_indices, J_indices, values, size(X)...)\n",
    "    return X_masked, y_sparse\n",
    "end\n",
    "\n",
    "X_train_masked, y_train_masked = mask_input_sparse(X_train)\n",
    "X_test_masked, y_test_masked = mask_input_sparse(X_test)\n",
    "\n",
    "function loss_sparse(model, x, y_sparse_batch, mode)\n",
    "    logits = model(x)  # (n_classes, seq_len, batch_size)\n",
    "    \n",
    "    rows_cpu, cols_cpu, vals_cpu = findnz(y_sparse_batch)\n",
    "    \n",
    "    if isempty(rows_cpu)\n",
    "        return 0.0f0\n",
    "    end\n",
    "    \n",
    "    rows_gpu = cu(rows_cpu)\n",
    "    cols_gpu = cu(cols_cpu) \n",
    "    vals_gpu = cu(vals_cpu)\n",
    "    \n",
    "    batch_size = size(logits, 3)\n",
    "    seq_len = size(logits, 2)\n",
    "    \n",
    "    linear_indices = (cols_gpu .- 1) .* seq_len .+ rows_gpu\n",
    "    \n",
    "    logits_reshaped = reshape(logits, size(logits, 1), :) # (n_classes, seq_len * batch_size)\n",
    "    masked_logits = logits_reshaped[:, linear_indices]  # (n_classes, n_masked)\n",
    "    \n",
    "    y_oh = Flux.onehotbatch(vals_gpu, 1:n_classes)\n",
    "    \n",
    "    if mode == \"train\"\n",
    "        return Flux.logitcrossentropy(masked_logits, y_oh)\n",
    "    elseif mode == \"test\"\n",
    "        return Flux.logitcrossentropy(masked_logits, y_oh), masked_logits, vals_gpu\n",
    "    end\n",
    "end\n",
    "\n",
    "function get_sparse_batch(y_sparse, start_idx, end_idx)\n",
    "    rows, cols, vals = findnz(y_sparse[:, start_idx:end_idx])\n",
    "    cols_adjusted = cols\n",
    "    batch_sparse = sparse(rows, cols_adjusted, vals, size(y_sparse, 1), end_idx - start_idx + 1)\n",
    "    \n",
    "    return batch_sparse\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e28e3c",
   "metadata": {},
   "source": [
    "** with the above code, 11mins 1 epoch (see github @ this time for other params) VS. 11mins 1 epoch for dense representations.\n",
    "can revisit later if there is found to be memory bottlenecks @ matrix operations, however for now the dense is sufficient b/c:\n",
    "https://www.reddit.com/r/Julia/comments/108g5ou/when_is_it_worth_working_with_sparse_matrices/\n",
    "https://medium.com/data-science/sparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71\n",
    "- above state that there should be about 1% or less sparsity for GPU sparse matrices to be efficient\n",
    "- due to the need to repeatedly transfer data between CPU and GPU and sparse slicing operations in batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29a6d02",
   "metadata": {},
   "source": [
    "june 12, 2025 trying dynamic masking;\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981827d4",
   "metadata": {},
   "source": [
    "**RoBERTa shows that masking a different subset every epoch already helps, \n",
    "but recent work finds that decreasing the rate during training is even better \n",
    "(to try later!!! aka scheduler)\n",
    "\n",
    "***dynamic on the train, static on the test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6fe77",
   "metadata": {},
   "source": [
    "jun 16, 2025 - tried to only mask 1 position; if it can't predict just the missing # from 1-978, then it's dumb.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb01a0e7",
   "metadata": {},
   "source": [
    "- the issue here might be that the test mask is different from the train mask; \n",
    "thus w/o dynamic masking, the train learns a single value each time, and the test provides a new value not seen before..?\n",
    "\n",
    "also trying to profile the memory b/c 25min per epoch is way too long;\n",
    "Profile.Allocs.@profile sample_rate=1 begin/end is taking wayyyyy too long too (~3hrs so far) to run in the REPL.. maybe\n",
    "is there another way?\n",
    "\n",
    "*also what takes 26h on kraken takes 4h on smaug...\n",
    "there seems to be slightly better trainval loss using higher embed dim --> try higher dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5c7136",
   "metadata": {},
   "source": [
    "jun 20, 2025 - based on the results from smaug, 2025-06-19_21-48\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107b8d3",
   "metadata": {},
   "source": [
    "there seems to be an issue with learning diff masked tokens (1 per sample) across the whole dataset\n",
    "it works if we want to do the same say, 5 masks across the whole dataset (albeit at only a 84% accuracy for some reason)\n",
    "1. double check masking function - ensure that it is correct\n",
    "2. scale up learning rate..? not sure what else to do here\n",
    "the training masks have sufficinet examples to learn from i think (60 per label) so what is going wrong?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1b0b2",
   "metadata": {},
   "source": [
    "jul 24, 2025 - speed/memory optimization\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac89a91",
   "metadata": {},
   "source": [
    "- lux.jl is equivalent of jax in python (uses xla backend)\n",
    "- unrelated, but wb a splitted data struct..? (what lea uses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d657f",
   "metadata": {},
   "source": [
    "jul 25, 2025 - exp masking...?\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d397d7bf",
   "metadata": {},
   "source": [
    "1. can you embed counts? how would that work\n",
    "2. needs to be a regression output (1 value) rather than a vector of probabilities per class?\n",
    "3. loss needs to be defined differently\n",
    "4. should it just be a dense network? does MHA work on "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae0d249",
   "metadata": {},
   "source": [
    "jul 30, 2025 - exp masking\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a574d2d",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Dense layer instead of Flux.Embedding \n",
    "    - this is b/c below static vs. dynamic Embeddings\n",
    "    - dense layer stores f(x) = Wx + b, where W and b are learned during training for all genes/samples\n",
    "    - W is a matrix, b is a vector and these are multiplied by the input x to output f(x) the embeddign vector \n",
    "- MHA still works with non-tokenized values!\n",
    "    - model learns the meaning of the expression levels rather than the gene identity/ranks?\n",
    "- mask token should be 0.0 after normalizing input\n",
    "- MSE loss\n",
    "- regression output to 1 value\n",
    "\n",
    "Static Embeddings (like Word2Vec or Flux.Embedding): \n",
    "- The model learns one single, fixed vector for each unique token (e.g., the word \"bank\"). \n",
    "- The goal is to learn the meaning of the token itself by averaging its usage across thousands of different contexts.\n",
    "Dynamic Feature Representation (Your Model): \n",
    "- Your model does not learn a static vector for \"Gene 1\". \n",
    "- Instead, it learns a function that maps any given expression value to a vector representation.\n",
    "\n",
    "9PM - edit to exp masking\n",
    "- had to move loss calc to before model update - resulted in lower loss in test than train\n",
    "- change masking val to -1, apparenlty there are exp elvels of 0 in the original dataset?\n",
    "\n",
    "***shoudl make a plot similar to this scatter plot for check_error.jl file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93ac6e4",
   "metadata": {},
   "source": [
    "aug 1, 2025 - pre lab-meeting\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5948cb",
   "metadata": {},
   "source": [
    "***need to fix the logging of params for predstrues.csv and params.txt ; didn't save in the last couple runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbd354f",
   "metadata": {},
   "source": [
    "aug 4, 2025 - debug + seb loss issue\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0740cc47",
   "metadata": {},
   "source": [
    "- runnign on GPU 2 for indef run - rerun for seb\n",
    "- need to \n",
    "    1. figure out logging for test/train - is it diff than what's in the slides?\n",
    "    2. debug original structure typing\n",
    "    3. fix logging of params for predstrues.csv and params.txt when doing input comparison plots\n",
    "    4. fix progressbar for indef_run.jl (why is it out of 628, repeats for each epoch???)\n",
    "\n",
    "- 08-04 run is original test/loss definitions from creating 720ep graph\n",
    "- changed code to use mask_transf code in the while loop\n",
    "- so Flux.withgradient = 1st loss calc --> update! --> second loss calc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b97cb",
   "metadata": {},
   "source": [
    "aug 5, 2025 - debug\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b30855",
   "metadata": {},
   "source": [
    "original structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85463f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PosEnc\n",
    "    pe_matrix::CuArray{Float32,2}\n",
    "end\n",
    "\n",
    "function PosEnc(embed_dim::Int, max_len::Int) # max_len is usually maximum length of sequence but here it is just len(genes)\n",
    "    pe_matrix = Matrix{Float32}(undef, embed_dim, max_len)\n",
    "    for pos in 1:max_len, i in 1:embed_dim\n",
    "        angle = pos / (10000^(2*(div(i-1,2))/embed_dim))\n",
    "        if mod(i, 2) == 1\n",
    "            pe_matrix[i,pos] = sin(angle) # odd indices\n",
    "        else\n",
    "            pe_matrix[i,pos] = cos(angle) # even indices\n",
    "        end\n",
    "    end\n",
    "    return PosEnc(cu(pe_matrix))\n",
    "end\n",
    "\n",
    "Flux.@functor PosEnc\n",
    "\n",
    "function (pe::PosEnc)(input::Float32Matrix3DType)\n",
    "    seq_len = size(input,2)\n",
    "    return input .+ pe.pe_matrix[:,1:seq_len] # adds positional encoding to input embeddings\n",
    "end\n",
    "\n",
    "### building transformer section\n",
    "\n",
    "struct Transf\n",
    "    mha::Flux.MultiHeadAttention\n",
    "    att_dropout::Flux.Dropout\n",
    "    att_norm::Flux.LayerNorm # this is the normalization aspect\n",
    "    mlp::Flux.Chain\n",
    "    mlp_norm::Flux.LayerNorm\n",
    "end\n",
    "\n",
    "function Transf(\n",
    "    embed_dim::Int, \n",
    "    hidden_dim::Int; \n",
    "    n_heads::Int, \n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    mha = Flux.MultiHeadAttention((embed_dim, embed_dim, embed_dim) => (embed_dim, embed_dim) => embed_dim, \n",
    "                                    nheads=n_heads, \n",
    "                                    dropout_prob=dropout_prob\n",
    "                                    )\n",
    "\n",
    "    att_dropout = Flux.Dropout(dropout_prob)\n",
    "    \n",
    "    att_norm = Flux.LayerNorm(embed_dim)\n",
    "    \n",
    "    mlp = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => hidden_dim, gelu),\n",
    "        Flux.Dropout(dropout_prob),\n",
    "        Flux.Dense(hidden_dim => embed_dim),\n",
    "        Flux.Dropout(dropout_prob)\n",
    "        )\n",
    "    mlp_norm = Flux.LayerNorm(embed_dim)\n",
    "\n",
    "    return Transf(mha, att_dropout, att_norm, mlp, mlp_norm)\n",
    "end\n",
    "\n",
    "Flux.@functor Transf\n",
    "\n",
    "function (tf::Transf)(input::Float32Matrix3DType) # input shape: embed_dim × seq_len × batch_size\n",
    "    normed = tf.att_norm(input)\n",
    "    atted = tf.mha(normed, normed, normed)[1] # outputs a tuple (a, b)\n",
    "    att_dropped = tf.att_dropout(atted)\n",
    "    residualed = input + att_dropped\n",
    "    res_normed = tf.mlp_norm(residualed)\n",
    "\n",
    "    embed_dim, seq_len, batch_size = size(res_normed)\n",
    "    reshaped = reshape(res_normed, embed_dim, seq_len * batch_size) # dense layers expect 2D inputs\n",
    "    mlp_out = tf.mlp(reshaped)\n",
    "    mlp_out_reshaped = reshape(mlp_out, embed_dim, seq_len, batch_size)\n",
    "    \n",
    "    tf_output = residualed + mlp_out_reshaped\n",
    "    return tf_output\n",
    "end\n",
    "\n",
    "### full model as << ranked data --> token embedding --> position embedding --> transformer --> classifier head >>\n",
    "\n",
    "struct Model\n",
    "    embedding::Flux.Embedding\n",
    "    pos_encoder::PosEnc\n",
    "    pos_dropout::Flux.Dropout\n",
    "    transformer::Flux.Chain\n",
    "    classifier::Flux.Chain\n",
    "end\n",
    "\n",
    "function Model(;\n",
    "    input_size::Int,\n",
    "    embed_dim::Int,\n",
    "    n_layers::Int,\n",
    "    n_classes::Int,\n",
    "    n_heads::Int,\n",
    "    hidden_dim::Int,\n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    embedding = Flux.Embedding(input_size => embed_dim)\n",
    "\n",
    "    pos_encoder = PosEnc(embed_dim, input_size)\n",
    "\n",
    "    pos_dropout = Flux.Dropout(dropout_prob)\n",
    "\n",
    "    transformer = Flux.Chain(\n",
    "        [Transf(embed_dim, hidden_dim; n_heads, dropout_prob) for _ in 1:n_layers]...\n",
    "        )\n",
    "\n",
    "    classifier = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => embed_dim, gelu),\n",
    "        Flux.LayerNorm(embed_dim),\n",
    "        Flux.Dense(embed_dim => n_classes)\n",
    "        )\n",
    "\n",
    "    return Model(embedding, pos_encoder, pos_dropout, transformer, classifier)\n",
    "end\n",
    "\n",
    "Flux.@functor Model\n",
    "\n",
    "function (model::Model)(input::IntMatrix2DType)\n",
    "    embedded = model.embedding(input)\n",
    "    encoded = model.pos_encoder(embedded)\n",
    "    encoded_dropped = model.pos_dropout(encoded)\n",
    "    transformed = model.transformer(encoded_dropped)\n",
    "    # pooled = dropdims(mean(transformed; dims=2), dims=2)\n",
    "    logits_output = model.classifier(transformed)\n",
    "    return logits_output\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ae10f",
   "metadata": {},
   "source": [
    "re-typed structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaccda7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PosEnc{U<:AbstractMatrix}\n",
    "    pe_matrix::U\n",
    "end\n",
    "\n",
    "function PosEnc(embed_dim::Int, max_len::Int) # max_len is usually maximum length of sequence but here it is just len(genes)\n",
    "    pe_matrix = Matrix{Float32}(undef, embed_dim, max_len)\n",
    "    for pos in 1:max_len, i in 1:embed_dim\n",
    "        angle = pos / (10000^(2*(div(i-1,2))/embed_dim))\n",
    "        if mod(i, 2) == 1\n",
    "            pe_matrix[i,pos] = sin(angle) # odd indices\n",
    "        else\n",
    "            pe_matrix[i,pos] = cos(angle) # even indices\n",
    "        end\n",
    "    end\n",
    "    return PosEnc(pe_matrix)\n",
    "end\n",
    "\n",
    "Flux.@functor PosEnc\n",
    "\n",
    "function (pe::PosEnc)(input::Float32Matrix3DType)\n",
    "    seq_len = size(input,2)\n",
    "    return input .+ pe.pe_matrix[:,1:seq_len] # adds positional encoding to input embeddings\n",
    "end\n",
    "\n",
    "### building transformer section\n",
    "\n",
    "struct Transf{MHA<:Flux.MultiHeadAttention, D<:Flux.Dropout, LN<:Flux.LayerNorm, C<:Flux.Chain}\n",
    "    mha::MHA\n",
    "    att_dropout::D\n",
    "    att_norm::LN\n",
    "    mlp::C\n",
    "    mlp_norm::LN\n",
    "end\n",
    "\n",
    "function Transf(\n",
    "    embed_dim::Int, \n",
    "    hidden_dim::Int; \n",
    "    n_heads::Int, \n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    mha = Flux.MultiHeadAttention((embed_dim, embed_dim, embed_dim) => (embed_dim, embed_dim) => embed_dim, \n",
    "                                    nheads=n_heads, \n",
    "                                    dropout_prob=dropout_prob\n",
    "                                    )\n",
    "\n",
    "    att_dropout = Flux.Dropout(dropout_prob)\n",
    "    \n",
    "    att_norm = Flux.LayerNorm(embed_dim)\n",
    "    \n",
    "    mlp = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => hidden_dim, gelu),\n",
    "        Flux.Dropout(dropout_prob),\n",
    "        Flux.Dense(hidden_dim => embed_dim),\n",
    "        Flux.Dropout(dropout_prob)\n",
    "        )\n",
    "    mlp_norm = Flux.LayerNorm(embed_dim)\n",
    "\n",
    "    return Transf(mha, att_dropout, att_norm, mlp, mlp_norm)\n",
    "end\n",
    "\n",
    "Flux.@functor Transf\n",
    "\n",
    "function (tf::Transf)(input::Float32Matrix3DType) # input shape: embed_dim × seq_len × batch_size\n",
    "    normed = tf.att_norm(input)\n",
    "    atted, _ = tf.mha(normed, normed, normed) # outputs a tuple (a, b)\n",
    "    att_dropped = tf.att_dropout(atted)\n",
    "    residualed = input + att_dropped\n",
    "    res_normed = tf.mlp_norm(residualed)\n",
    "\n",
    "    embed_dim, seq_len, batch_size = size(res_normed)\n",
    "    reshaped = reshape(res_normed, embed_dim, seq_len * batch_size) # dense layers expect 2D inputs\n",
    "    mlp_out = tf.mlp(reshaped)\n",
    "    mlp_out_reshaped = reshape(mlp_out, embed_dim, seq_len, batch_size)\n",
    "    \n",
    "    tf_output = residualed + mlp_out_reshaped\n",
    "    return tf_output\n",
    "end\n",
    "\n",
    "struct Model{E<:Flux.Embedding, P<:PosEnc, D<:Flux.Dropout, T<:Flux.Chain, C<:Flux.Chain}\n",
    "    embedding::E\n",
    "    pos_encoder::P\n",
    "    pos_dropout::D\n",
    "    transformer::T\n",
    "    classifier::C\n",
    "end\n",
    "\n",
    "function Model(;\n",
    "    input_size::Int,\n",
    "    embed_dim::Int,\n",
    "    n_layers::Int,\n",
    "    n_classes::Int,\n",
    "    n_heads::Int,\n",
    "    hidden_dim::Int,\n",
    "    dropout_prob::Float64\n",
    "    )\n",
    "\n",
    "    embedding = Flux.Embedding(input_size => embed_dim)\n",
    "\n",
    "    pos_encoder = PosEnc(embed_dim, input_size)\n",
    "\n",
    "    pos_dropout = Flux.Dropout(dropout_prob)\n",
    "\n",
    "    transformer = Flux.Chain(\n",
    "    (Transf(embed_dim, hidden_dim; n_heads, dropout_prob) for _ in 1:n_layers)...\n",
    "    )\n",
    "\n",
    "    classifier = Flux.Chain(\n",
    "        Flux.Dense(embed_dim => embed_dim, gelu),\n",
    "        Flux.LayerNorm(embed_dim),\n",
    "        Flux.Dense(embed_dim => n_classes)\n",
    "        )\n",
    "\n",
    "    return Model(embedding, pos_encoder, pos_dropout, transformer, classifier)\n",
    "end\n",
    "\n",
    "Flux.@functor Model\n",
    "\n",
    "function (model::Model)(input::T) where {T<:IntMatrix2DType} \n",
    "    # there is an issue here - where type is Any from the Flux portion\n",
    "    # now - if Flux is causing issues, go into source code and redefine as above:\n",
    "    # (m::Embedding)(x::T) where {T<:AbstractArray} = reshape(m(vec(x)), :, size(x)...), copied from Flux source code\n",
    "    # AND\n",
    "    # input::T where T<:type, allows it to be distinguished as a subtype of the input type\n",
    "    # should theoretically be able to avoid Anys, and be type-stable!\n",
    "    embedded = model.embedding(input)\n",
    "    encoded = model.pos_encoder(embedded)\n",
    "    encoded_dropped = model.pos_dropout(encoded)\n",
    "    transformed = model.transformer(encoded_dropped)\n",
    "    pooled = dropdims(mean(transformed; dims=2), dims=2)\n",
    "    logits_output = model.classifier(pooled)\n",
    "    return logits_output\n",
    "    return embedded\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee13bed",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- need to \n",
    "    1. ~~re-run with fixed typing~~ faster.jl running on kraken gpu 1\n",
    "\n",
    "    2. ~~figure out why test is better than train for loss/accuracy (potentially change in indef_run code or run mask_transf code for x epochs if test is only better in indef_run and not mask_transf)~~ ~~indef_run.jl code changed, running new on smaug gpu 2 ONCE OLD_INDEF_RUN.JL gets to 40!!! --> running new one now! what was the diff bruh~~ doen + clarified fix, see indef_masked_rankings 08-04 vs. 08-05. issue was the withgradient (AGAIN!!!)\n",
    "\n",
    "    3. ~~fix param logging for exp_transf + mask_transf (predstrues.csv, params.txt)~~\n",
    "\n",
    "    4. ~~fix progressbar for indef_run.jl~~ removed progress bar lol\n",
    "\n",
    "    5. ~~fix scatter plot for mask_transf comparison~~ ~~mask_transf_err.jl running on kraken gpu 0~~ done, see masked_rankings/2025-08-05\n",
    "\n",
    "    6. ~~x-bin for exp_transf comparison~~ ~~exp_transf.jl running on smaug gpu 3~~ done, see masked_expression/2025-08-05\n",
    "\n",
    "    7. reorganize exp, mask, indef, faster for tomorrow\n",
    "\n",
    "    8. put fxns/structs into separate src files! more organized.\n",
    "\n",
    "- 08-04 run is original test/loss calculations from creating 720ep graph\n",
    "- 08-05 run is updated calculations from mask_transf code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71af32f5",
   "metadata": {},
   "source": [
    "aug 6, 2025 - recap\n",
    "="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ad1644",
   "metadata": {},
   "source": [
    "asap:\n",
    "- ~~exp_transf.jl running on kraken 0 (10ep x-bin)~~ done\n",
    "- ~~mask_transf_err.jl running on smaug 3 (10ep heatmap)~~ done\n",
    "\n",
    "still pending:\n",
    "- faster.jl running on kraken gpu 1 --> old code: 668774 ms, new code:\n",
    "    - terminated - need to fix code\n",
    "- reorganize exp, mask, indef, faster\n",
    "- put fxns/structs into separate src files! more organized.\n",
    "- ~~why not: exp transf run on untrt - kraken 0~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f18c7db",
   "metadata": {},
   "source": [
    "aug 12, 2025 - predicting the average, ensuring no repeats, fixing plots\n",
    "="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5098b",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- ~~redo heatmap/x-bin into boxplots or hex-bin?~~\n",
    "    ~~- exp on kraken 0, rank on smaug 0~~\n",
    "    - longer rank run on smaug 0\n",
    "- ~~see if model is just prev the avg rather than acc learning (raw exp)~~\n",
    "    - in exp code, running 100ep on kraken 0 for comparison\n",
    "- ~~see if possible to ensure model has no repeats (via permutations, inductive bias, pointer networks)~~\n",
    "    - trying on smaug 1 (need to clean up and understand tho)\n",
    "- put fxns/structs into separate src files! more organized.\n",
    "- make new diagrams! (look into CLE token)\n",
    "- do test iwthout pretrain to see if masking even helps\n",
    "- faster.jl ; compare memory usage still high!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fa216",
   "metadata": {},
   "source": [
    "aug 14, 2025 - for ensuring no repeats\n",
    "="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b80874",
   "metadata": {},
   "source": [
    "a pointer network is designed to select its output from the elements that are **present in the input sequence**. however, the correct answer (the masked number) is the one element that is explicitly absent from the input.\n",
    "\n",
    "https://kierszbaumsamuel.medium.com/pointer-networks-what-are-they-c3cb68fae076#:~:text=Notice%20how%20they%20are%20placed,-%20output%20dictionary%3A\n",
    "\n",
    "https://arxiv.org/abs/1506.03134 - pointer networks\n",
    "\n",
    "https://arxiv.org/abs/2006.06380 - pointer graph networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf26e77",
   "metadata": {},
   "source": [
    "alternatively:\n",
    "\n",
    "can have two sets of inputs:\n",
    "- context: masked sequence [1, 2, ..., MASK, 79, ...].\n",
    "- candidate: complete, unmasked set of all possible tokens [1, 2, ..., 978].\n",
    "\n",
    "where:\n",
    "- encoder processes the context input to understand what's missing.\n",
    "- decoder or attention mechanism then uses this context to point to the correct token within the candidate input.\n",
    "\n",
    "thus, the model isn't pointing to the sequence it was given but to a complete \"dictionary\" of possibilities, using the masked sequence to figure out which item in the dictionary is the right one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee8c15",
   "metadata": {},
   "source": [
    "is this realistically better?\n",
    "\n",
    "standard transf:\n",
    "- model must produce a single vector for the [MASK] token that, after going through one final linear layer, can be classified as 78\n",
    "- vector has to implicitly encode the identity \"78\"\n",
    "- model learns a complex, abstract function to map from context to an identity\n",
    "\n",
    "pointer:\n",
    "- model must produce a query vector for the [MASK] token\n",
    "- query's job is to be more similar to the vector for 78 in the candidate set than to any other number's vector\n",
    "- forces the model to learn a shared, consistent embedding space for all numbers\n",
    "- representation for 78 must be similar whether it's in the input or in the candidate list\n",
    "- encourages the model to learn the concept of \"78-ness\" in a way that is directly comparable to the concepts of \"77-ness\" and \"79-ness.\"\n",
    "\n",
    "THUS, leads to a more structured and relational embedding space..?\n",
    "\n",
    "*useful for things like travelling salesman problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bec63f",
   "metadata": {},
   "source": [
    "similar to: https://arxiv.org/abs/2005.11401\n",
    "\n",
    "RAG models improve language models by first using the input to retrieve relevant documents from a vast database (like Wikipedia). The model then uses both the original input and the retrieved documents to generate a final answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26396b47",
   "metadata": {},
   "source": [
    "aug 18, 2025 - currently\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf90c59",
   "metadata": {},
   "source": [
    "running rn:\n",
    "- smaug 0: ranking long run (300ep) with updated metrics plotting\n",
    "\n",
    "- ~~smaug 1: exp long run (300ep) wiht updated metrics plotting (and checking if just pred avg)~~\n",
    "    - done, not just pred avg; did well! (2025-08-18_16-03)\n",
    "\n",
    "working on rn:\n",
    "- ~~seeing if model is just predicting the average~~\n",
    "    - i think done, seems that model is doing better than average (2025-08-18_16-03)\n",
    "- investigating pointer networks and/or permutations, inductive bias\n",
    "- lux.jl to decrease mem allocs?\n",
    "- do test w/ and w/o pretrain to see if masking even helps\n",
    "    - should save weights of model as well (or model itself somehow for downstream applications)\n",
    "- reorganize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07f73c",
   "metadata": {},
   "source": [
    "aug 19, 2025 - no repeats (more options)\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe47c5b",
   "metadata": {},
   "source": [
    "some ideas:\n",
    "- inductive bias on the logits before softmax\n",
    "- constrained beam search (decoder-side control)\n",
    "- energy-based; adding a penalty for choosing a forbidden value\n",
    "    - allows for soft penalties .. not sure if this is good or bad\n",
    "- ~~output as a set (Set Transformers, DeepSets, or Determinantal Point Processes (DPPs))~~\n",
    "    - implies order doesn't matter?\n",
    "- ~~ILP/SAT decoding??~~\n",
    "    - not scalable\n",
    "- symbolic rule-based filter or constraint-satisfaction layer\n",
    "    - like logic tensor networks\n",
    "- copy/generate models\n",
    "    - also soft penalties; can forbid copy mode (in input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d68e6a",
   "metadata": {},
   "source": [
    "ones w/ hard penalties:\n",
    "- inductive bias\n",
    "    - isn't this what i am already doing via masking?\n",
    "    - no; rn i have data-level masking. inductive bias is prediction-level masking!\n",
    "- constrained beam search\n",
    "    - does not scale well too 100k samples\n",
    "- neural/symbolic hybrid\n",
    "    - 2 stage process - not sure how to backprop thru symbolic rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb0f11c",
   "metadata": {},
   "source": [
    "inductive bias:\n",
    "- before applying softmax (in loss function, after compute logits), you should set the logits for:\n",
    "    - any gene IDs already present in the input for that sample (so they can’t be predicted), and\n",
    "    - any gene IDs already predicted in previous decoding steps (if doing sequential prediction).\n",
    "- to -Inf (or a very negative number).\n",
    "- this ensures that the probability of those disallowed gene IDs is exactly 0.\n",
    "- rn code allows the model to assign probability mass to any of the n_classes outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2fada2",
   "metadata": {},
   "source": [
    "constrained beam search:\n",
    "- ~~greedy~~\n",
    "    - At each position t (row), you look at the logits for that position and immediately pick the single best legal option (argmax after masking forbidden genes).\n",
    "    - If enforce_unique=true, you also remove that choice from future positions.\n",
    "    - The decision is locally optimal (best at that timestep given constraints).\n",
    "- compact\n",
    "    - Instead of committing immediately, you keep the top-k partial hypotheses (“beams”) as you decode.\n",
    "    - At each timestep, every beam expands to multiple candidates (respecting constraints), then you prune back down to the top-k by cumulative score.\n",
    "    - After the last position, you return the best sequence.\n",
    "    - The decision is globally optimized across the sequence within beam budget."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38d669",
   "metadata": {},
   "source": [
    "neural/symbolic hybrid\n",
    "- split prediction into two modules:\n",
    "    - neural module: proposes a ranked list of candidate genes\n",
    "    - symbolic module: enforces your biological or structural rules\n",
    "- this is like a two-stage pipeline: model suggests --> rules finalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f54fa",
   "metadata": {},
   "source": [
    "SO FINAL IDEAS TO IMPLEMENT:\n",
    "- compact beam search\n",
    "    - 2017: https://arxiv.org/pdf/1704.07138 (grid beam search)\n",
    "    - 2018: https://arxiv.org/pdf/1804.06609 (dynamic beam allocation)\n",
    "    - explanation: https://huggingface.co/blog/constrained-beam-search\n",
    "- neural/symbolic constraints\n",
    "    - 2021: https://arxiv.org/pdf/2103.17232 (first introduction)\n",
    "    - 2024: https://arxiv.org/pdf/2410.20957 (logical constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe110bb",
   "metadata": {},
   "source": [
    "aug 20, 2025 - post-seb meeting\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b19ff",
   "metadata": {},
   "source": [
    "- comparison plots\n",
    "    - ~~waiting on smaug 0~~ done\n",
    "    - ~~- show distribution of true values in histogram; has very little values below 5 so we can ignore that~~ done; explain the distribution correlation with the accuracy as well!\n",
    "    - ~~- plot both boxplot and hist on top of one another with same axes~~\n",
    "    - ~~- see if possible to top whisker at 0.9 quartile and bottom whisker at 0.1 quartile; thus 10% of pts at upper and lower whisker rather than extrapolating data~~ done; not much different but easier to explain plot\n",
    "    - ~~- currently, what is used is IQR of 1.5 for whisker length~~\n",
    "    - ~~- try rangebars or @recipe macro on CairoMakie~~\n",
    "- just predicting avg expression?\n",
    "    - can also show this in the boxplot by adding an X where the average is (maybe no need)\n",
    "    - ~~- also, double check if we are comparing the hexbin of true values vs. average of predicted values (correct) or average of true values (incorrect)~~ correct!\n",
    "- improving ranked model\n",
    "    - ensure parameters are on the same scale generally; such that the exp and rank models have the same capacity\n",
    "    - since the rank model has less info in the input, it doing just as good as the exp model is sufficient\n",
    "    - for eval:\n",
    "        - can do a downstream task, OR\n",
    "        - we can compare actual values of expression model output to rank model output without needing significant inductive biases applied or downstream tasks\n",
    "        - need to look into how to have input = rank, output = expression value (for ranked model)\n",
    "        - ex. quantile normalization (w/ same distribution?)\n",
    "- slurm connect\n",
    "- review how exp model works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c27760",
   "metadata": {},
   "source": [
    "sep 3, 2025 - check-in\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ed6b8",
   "metadata": {},
   "source": [
    "plotting\n",
    "- new_boxplot.png compared to old boxplot in aug22 ppt\n",
    "- box_hex.png\n",
    "\n",
    "improving ranked model\n",
    "- opt 1: input = rank, output = expression val\n",
    "    - A. concatenate/add ranked embeddings + raw expression embeddings\n",
    "        - but this means that the rank model isn't 100% a rank model\n",
    "    - B. change to regression task w/ only model structure, fwd pass, and loss\n",
    "        - structure: classifier --> Flux.Chain(embed_dim => hidden_dim => 1)\n",
    "            - 1 for each value masked; loss is calculated on each masked value individually\n",
    "        - fwd pass: somewhat same, or we can use pooling?\n",
    "            - involves taking the mean across the sequence length\n",
    "        - loss: logit cross-entropy --> MSE?\n",
    "        - similar to what is already done in exp_transf.jl, just with diff input?\n",
    "    - C. quantile normalization\n",
    "        - replaces the integer rank with float of average expression for each gene\n",
    "        - then can convert that into a rank but maintain the previous raw values for y-labels?\n",
    "        - then predict singular value (Flux.Chain => 1) for expression prediction?\n",
    "        - does this generalize too much tho?\n",
    "- opt 2: constraint stuff\n",
    "    - ex. constrained beam search, neural/symbolic hybrid\n",
    "\n",
    "misc\n",
    "- downstream task?\n",
    "- slurm connect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d36de83",
   "metadata": {},
   "source": [
    "sep 4, 2025 - post- seb meeting\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e27511",
   "metadata": {},
   "source": [
    "plotting\n",
    "- get correlation of avg hexbin for comparison\n",
    "- update entropu graphs for comparison\n",
    "    - ex. have accuracy/entropy vs. rank and vs. expression\n",
    "        - look at density of probabilty for the expression one (but also seb said maybe no for this)\n",
    "        - re-running error vs rank for trt dataset on kraken 0\n",
    "    - see if the movement in position is correlated iwth cell type as well!\n",
    "    - look into more explanation stuff like this^^^\n",
    "\n",
    "dataset\n",
    "- do LINCS treated and untreated dataset\n",
    "    - for exmaple, if we have larger dataset (by x10) then do # epochs / 10 (300 ep on untrt, 30 ep on trt/untrt); thus we have the same # of gradient steps\n",
    "    - try to run a small subsample first for speed and testing\n",
    "    - rank currently running on smaug 0, exp currenty running on smaug 1 (09-07)\n",
    "\n",
    "task\n",
    "- predict cell line then predict average expression of that cell line\n",
    "\n",
    "models\n",
    "- use FNN/MLP for expression profile with same number of parameters as ranking transformer\n",
    "    - as the initial comparison\n",
    "\n",
    "evaluation\n",
    "- find more differences? but masking task might be sufficient\n",
    "- look into autoencoders which ask to recover values from 0 (ex. denoising autoencoder); is this similar to masking alr tho?\n",
    "\n",
    "validating it is acc learning\n",
    "- could remove the mean expression for each gene before input; thus it is definitely not just learning the average; but instead if avg is 0 then it is definitley learning properly\n",
    "    - this also means i'd need to change the MASK from -1 to something else (ex. a collection of 0s and 1s..?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20300b8",
   "metadata": {},
   "source": [
    "sep 9, 2025 - prep ppt for meeting fri\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e48bb3",
   "metadata": {},
   "source": [
    "for ppt:\n",
    "- introduction, model, how masking works,etc\n",
    "- rank vs. exp comaprison\n",
    "    - plot of train/test loss\n",
    "    - plot of boxplots of pred vs true\n",
    "        - with a histogram for the exp val\n",
    "        - no histogram needed for the rank; it is uniform distr\n",
    "    - also error only for rank (should there be error for exp as well???)\n",
    "    - if smaug done in time, show big params vs smol params runs (30ep vs 5ep)\n",
    "- reasoning results\n",
    "    - plot of entropies in dataset\n",
    "    - plot of prediction error by value\n",
    "    - plot of avg hexbin???\n",
    "- new task!\n",
    "- new model..?\n",
    "***\n",
    "\n",
    "plotting\n",
    "- updated entropy graphs to include trt data\n",
    "    - rank: done\n",
    "    - exp: done\n",
    "        - using bins of size 0.01 and 0.1 for discretization\n",
    "- to compare to entropy graphs, re-run on trt data for error vs:\n",
    "    - rank: **running on kraken 0, 5ep**\n",
    "    - exp: **running on kraken 1, 5ep**\n",
    "- plot cell type against error\n",
    "    - cell type vs. error: ?\n",
    "    - cell type vs. entropy: done\n",
    "- get explanations for above\n",
    "- get correlation of avg hexbin for comparison (not urgent)\n",
    "\n",
    "dataset --> changed from untrt only to trt and untrt\n",
    "- long run with big params\n",
    "    - rank: **running on smaug 0, 30ep**\n",
    "    - exp: **running on smaug 1, 30ep**\n",
    "\n",
    "task\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdbc4cf",
   "metadata": {},
   "source": [
    "sep 12, 2025 - parameter checking, organizing todo\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1d9d3",
   "metadata": {},
   "source": [
    "1. comparing input results\n",
    "- be able to explain projection on raw vs. embedding on rank\n",
    "- check: does exp model have only 128 params per input vector via Wx+b (128\\*1) while the rank model has 128\\*978 params where each value gets projected the weight dimension?\n",
    "    - this would result in less complexity for the embedding model, which means that the embedding model has a disadvantage\n",
    "    - **EXP MODEL # PARAMS: 649,649**\n",
    "        - via total_params = sum(length, Flux.params(model))\n",
    "        - embed_dim = 128, hidden_dim = 236, n_heads = 2, n_layers = 4\n",
    "    - **RANK MODEL # PARAMS: 921,426**\n",
    "        - embed_dim = 128, hidden_dim = 256, n_heads = 2, n_layers = 4\n",
    "        - params length = 56\n",
    "        - (128, 979)(128, 979)(128, 128)(128, 128)(128, 128)(128, 128)(128,)(128,)(256, 128)(256,)(128, 256)(128,)(128,)(128,)(128, 128)(128, 128)(128, 128)(128, 128)(128,)(128,)(256, 128)(256,)(128, 256)(128,)(128,)(128,)(128, 128)(128, 128)(128, 128)(128, 128)(128,)(128,)(256, 128)(256,)(128, 256)(128,)(128,)(128,)(128, 128)(128, 128)(128, 128)(128, 128)(128,)(128,)(256, 128)(256,)(128, 256)(128,)(128,)(128,)(128, 128)(128,)(128,)(128,)(978, 128)(978,)\n",
    "\n",
    "2. entropy vs. error graphs **--> APPLY TO THE 30EP RUN**\n",
    "- rank:\n",
    "    - take mean/avg error per rank instead for better visualization\n",
    "- expression:\n",
    "    - double check the p(x) distribution of the bins; are they fitting in properly or are some bins have 0?\n",
    "    - do std. dev of expression (or potentially interquartile distance) rather than entropy - this has better representation of variation\n",
    "3. next steps\n",
    "- MLP as an autoencoder; train the beginning then prediction head at the end is different - to sub in for downstream task\n",
    "    - aka pretraining an MLP?\n",
    "    - see denoising autoencoder\n",
    "- stacked ass autoencoder vs. transformer with same number of parameters (same degrees of freedom)\n",
    "    - if autoencoder better then yay! if transformer better then boo.\n",
    "- see leo's bottleneck stuff\n",
    "- look at benchmarks\n",
    "    - aka lea's; predicting gene expression from a different cell line and same perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a726174",
   "metadata": {},
   "source": [
    "currently running: 30ep exp run on smaug 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac4561",
   "metadata": {},
   "source": [
    "masked model\n",
    "-\n",
    "julia> model.classifier\n",
    "Chain(\n",
    "  Dense(128 => 128, gelu_tanh),         # 16_512 parameters\n",
    "  LayerNorm(128),                       # 256 parameters\n",
    "  Dense(128 => 978),                    # 126_162 parameters\n",
    ")                   # Total: 6 arrays, 142_930 parameters, 992 bytes.\n",
    "\n",
    "julia> model.embedding\n",
    "Embedding(979 => 128)  # 125_312 parameters\n",
    "\n",
    "julia> model.pos_encoder\n",
    "PosEnc(Float32[0.84147096 0.9092974 … -0.8218694 -0.92342377; 0.5403023 -0.41614684 … -0.56967604 0.38378194; … ; 0.0001154782 0.0002309564 … 0.11269774 0.11281249; 1.0 1.0 … 0.99362934 0.9936163])\n",
    "\n",
    "julia> model.transformer\n",
    "Chain(\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 256, gelu_tanh),     # 33_024 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(256 => 128),                # 32_896 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 256, gelu_tanh),     # 33_024 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(256 => 128),                # 32_896 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 256, gelu_tanh),     # 33_024 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(256 => 128),                # 32_896 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 256, gelu_tanh),     # 33_024 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(256 => 128),                # 32_896 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    ")                   # Total: 48 arrays, 527_872 parameters, 8.328 KiB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29172b1",
   "metadata": {},
   "source": [
    "exp model\n",
    "-\n",
    "julia> model.classifier\n",
    "Chain(\n",
    "  Dense(128 => 128, gelu_tanh),         # 16_512 parameters\n",
    "  LayerNorm(128),                       # 256 parameters\n",
    "  Dense(128 => 1, softplus),            # 129 parameters\n",
    ")                   # Total: 6 arrays, 16_897 parameters, 992 bytes.\n",
    "\n",
    "julia> model.projection\n",
    "Dense(1 => 128)     # 256 parameters\n",
    "\n",
    "julia> model.pos_encoder\n",
    "PosEnc(Float32[0.84147096 0.9092974 … 0.035307925 -0.8218694; 0.5403023 -0.41614684 … -0.9993765 -0.56967604; … ; 0.0001154782 0.0002309564 … 0.112583004 0.11269774; 1.0 1.0 … 0.99364233 0.99362934])\n",
    "\n",
    "julia> model.transformer\n",
    "Chain(\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 236, gelu_tanh),     # 30_444 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(236 => 128),                # 30_336 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 236, gelu_tanh),     # 30_444 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(236 => 128),                # 30_336 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 236, gelu_tanh),     # 30_444 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(236 => 128),                # 30_336 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    "  Transf(\n",
    "    MultiHeadAttention(128; nheads=2, dropout_prob=0.05),  # 65_536 parameters\n",
    "    Dropout(0.05),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "    Chain(\n",
    "      Dense(128 => 236, gelu_tanh),     # 30_444 parameters\n",
    "      Dropout(0.05),\n",
    "      Dense(236 => 128),                # 30_336 parameters\n",
    "      Dropout(0.05),\n",
    "    ),\n",
    "    LayerNorm(128),                     # 256 parameters\n",
    "  ),\n",
    ")                   # Total: 48 arrays, 507_312 parameters, 8.328 KiB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65547872",
   "metadata": {},
   "source": [
    "rank model:\n",
    "- \n",
    "- input:\n",
    "    - Embedding(979 => 128) layer\n",
    "    - creates a lookup table to convert each of 979 unique input tokens into a 128-dimensional vector\n",
    "    - 979×128=125,312 parameters\n",
    "- output:\n",
    "    - Dense(128 => 978) layer\n",
    "    -  takes the final 128-dimensional representation and projects it into 978 output values, corresponding to the probability of each token in the vocabulary\n",
    "    - 128×978+978=126,162 parameters\n",
    "\n",
    "exp model:\n",
    "- input:\n",
    "    - Dense(1 => 128) layer\n",
    "    - takes a single number and projects it into a 128-dimensional vector\n",
    "    - 1×128+128=256 parameters.\n",
    "- output: \n",
    "    - Dense(128 => 1, softplus)\n",
    "    - outputs a single number\n",
    "    - 128×1+1=129 parameters\n",
    "\n",
    "difference: ~250k parameters\n",
    "\n",
    "~~**ADDITIONALLY:~~ **--> FIXED**\n",
    "- masked: trasnf netwrok expands the dimension from 128 to 256 and then back to 128 (128 => 256 => 128)\n",
    "- exp: transf netwrok expands the dimension from 128 to 236 and then back to 128 (128 => 236 => 128)\n",
    "\n",
    "difference: ~20k parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53605f55",
   "metadata": {},
   "source": [
    "sep 15, 2025 - autoencoder research\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907d4827",
   "metadata": {},
   "source": [
    "https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf\n",
    "- stacked denoising autoencoders --> similar to masked pretrain objective\n",
    "    - difference:\n",
    "        - DAE = layer-wise pretrain; T = end-to-end pretrain\n",
    "        - DAE = local ft detection (neural net); T = global/bidirectional context awareness (via posenc too) (attention)\n",
    "    - similarity:\n",
    "        - adding noise = setting inputs to 0 = masking\n",
    "            - so one of the main things of the DAE is that it uses diff \"corruption\" methods; ex. Gaussian noise, masking (set to 0), salt/pepper noise (set to max/min val, uysualy either 0 or 1) \n",
    "            - reconstructing input = predicting identity of whatever was masked/hidden\n",
    "- for my project:\n",
    "    - essentially can use this as the \"MLP\" (try with/without the stacking)\n",
    "    - different reconstruction tasks other than masking; ex. Gaussian noise, salt/pepper noise?\n",
    "    - BUT should i be doing 1. exp DAE, 2. rank DAE, 3. exp transf, 4. rank transf?\n",
    "        - for comparison against both input aspects and architecture aspects?\n",
    "        - what else needs to be done in terms of supporting the idea that the baselines can perform just as well? (referencing https://www.nature.com/articles/s41592-025-02772-6#Sec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde7f969",
   "metadata": {},
   "source": [
    "reasoning:\n",
    "- https://arxiv.org/abs/2502.19718 (2025)\n",
    "    - information theory perspective on masked autoencoders\n",
    "\n",
    "**look into:**\n",
    "- can i use a DAE or MAE to compare against an MLM?\n",
    "    - shouldn't it be bidirectional like a transformer? or is it already?\n",
    "    - DAE: see above\n",
    "    - MAE: https://arxiv.org/pdf/1502.03509\n",
    "- what about contractive autoencoders: https://icml.cc/2011/papers/455_icmlpaper.pdf\n",
    "\n",
    "**some new stuff:**\n",
    "- https://www.nature.com/articles/s41598-025-96215-z\n",
    "- https://arxiv.org/abs/2505.22914"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b000ab",
   "metadata": {},
   "source": [
    "for tmo:\n",
    "- leo's bottleneck stuff\n",
    "- lea's benchmarking?\n",
    "- other benchmarking?\n",
    "- prep for meeting; other reasonings why to do or not to do DAE/MAE (specifically the quad-comparison as mentioned td above)\n",
    "- difference between AE and MLP or FNN? or same thing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d47e1b",
   "metadata": {},
   "source": [
    "sep 16, 2025 - organizing objectives\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab62e8",
   "metadata": {},
   "source": [
    "for input comparison:\n",
    "- i'm thinking of having exp vs. rank on a transformer and exp vs. rank on a FNN...?\n",
    "- currently the only difference i have between the inputs is that the degree of variation per token learned is a lot greater in the ranked input (measured in entropy) compared to the raw expression value input (measured in variance)\n",
    "\n",
    "for architecture comparison:\n",
    "- currently there are ~250k more parameters in the masked model (649,649 vs. 921,426)\n",
    "    - this is due to:\n",
    "        - rank model:\n",
    "            - input: Embedding(979 => 128) layer (979×128=125,312 params)\n",
    "            - output: Dense(128 => 978) layer (128×978+978=126,162 params) - +978 is due to # biases\n",
    "        - exp model:\n",
    "            - input: Dense(1 => 128) layer (1×128+128=256 params) - +128 is due to # biases\n",
    "            - output: Dense(128 => 1, softplus) (128×1+1=129 params) - +1 is due to # biases\n",
    "- mainly: FNN against MLM:\n",
    "    - should the FNN be 1 layer, DAE, or MAE? (or maybe try all 3?)\n",
    "\n",
    "update on plotting:\n",
    "- entropy vs. error graphs **--> APPLY TO THE 30EP RUN**\n",
    "    - rank:\n",
    "        - take mean/avg error per rank instead for better visualization\n",
    "    - expression:\n",
    "        - double check the p(x) distribution of the bins; are they fitting in properly or are some bins have 0?\n",
    "        - do std. dev of expression (or potentially interquartile distance) rather than entropy - this has better representation of variation\n",
    "- get correlation of avg hexbin for comparison in supplementary\n",
    "\n",
    "it was mentioned last wk:\n",
    "- look at benchmarks\n",
    "    - leo's; some bottleneck stuff?\n",
    "    - lea's; predicting gene expression from a different cell line and same perturbation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6057b6a1",
   "metadata": {},
   "source": [
    "for reference:\n",
    "- MLP\n",
    "    - supervised learning (vs. AE is unsupervised)\n",
    "    - input layer + hidden layer(s) + output layer; loss is calcualted by comparing output to true label\n",
    "- DAE\n",
    "    - corrupts input data and decoder reconstructs the original; loss is calculated between reconstructed form z and original input x\n",
    "    - https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf (stacked DAE)\n",
    "    - https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf (robust ft w/ DAEs)\n",
    "- MAE\n",
    "    - in images: randomly mask 75% of an image, encoding only the visible patches and decoder reconstructs the original\n",
    "    - outside of images:\n",
    "        - https://arxiv.org/abs/2309.13793 (ReMasker)\n",
    "            - original input has missing values; additional inputs are masked for AE to train on - then applied to original missing values\n",
    "        - https://arxiv.org/abs/2412.19152 (PMAE)***\n",
    "            - masks original input w/ a probability inversely related to a column's observation rate\n",
    "            - ensures that the model must learn more from rarer/less frequently observed features\n",
    "- CAE\n",
    "    - AE w/ small penalty to loss fxn to reduce sensitivity to small/local variations in the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d6635d",
   "metadata": {},
   "source": [
    "todo: \n",
    "- ~~does MAE exist outside of images/comp vision?~~ yes; see above\n",
    "- ~~wtf does MAE for distribution estimation do?~~ learns data distribution from reconstructing masked input\n",
    "- ~~finish why params are different~~\n",
    "- ~~redo error per rank graph~~ @ /home/golem/scratch/chans/lincs/plots/trt_and_untrt/masked_rankings/2025-09-11_08-26\n",
    "- ~~redo entropy (std dev instead) per exp graph (on untrt since trt still running)~~\n",
    "    - check bin distribution of expression error (on untrt since trt still running)\n",
    "- do avg exp of genes then sort from hgihest to lowest for comparison against rank error graph\n",
    "- look more into reasoning why input diff = terrible ranked input performance\n",
    "\n",
    "later:\n",
    "- avg hexbin correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2cd1b0",
   "metadata": {},
   "source": [
    "sep 17, 2025\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec406e",
   "metadata": {},
   "source": [
    "INPUT COMPARISON:\n",
    "- i'm thinking of having exp vs. rank on a transformer and exp vs. rank on a FNN...?\n",
    "- currently the only difference i have between the inputs is that the degree of variation per token learned is a lot greater in the ranked input (measured in entropy) compared to the raw expression value input (measured in variance)\n",
    "\n",
    "ARCHITECTURE COMPARISON:\n",
    "- currently there are ~250k more parameters in the masked model (649,649 vs. 921,426)\n",
    "    - this is due to:\n",
    "        - rank model:\n",
    "            - input: Embedding(979 => 128) layer (979×128=125,312 params)\n",
    "            - output: Dense(128 => 978) layer (128×978+978=126,162 params) - +978 is due to # biases\n",
    "        - exp model:\n",
    "            - input: Dense(1 => 128) layer (1×128+128=256 params) - +128 is due to # biases\n",
    "            - output: Dense(128 => 1, softplus) (128×1+1=129 params) - +1 is due to # biases\n",
    "- mainly: FNN against MLM:\n",
    "    - should the FNN be 1 layer, DAE, or MAE? (or maybe try all 3?)\n",
    "\n",
    "PLOTTING:\n",
    "- entropy vs. error graphs **--> APPLY TO THE 30EP RUN**\n",
    "    - rank: (trt-rankings)\n",
    "        - see rank_vs_avgerror (scatter vs line.png)\n",
    "    - expression: (infographs)\n",
    "        - see gene_exp_std_dev_trt.png (is this ok or is IQ distance a better representation?)\n",
    "    - sorted gene expression: (infographs)\n",
    "        - sorted_gene_mean_exp_trt.png for validation of sorting\n",
    "        - sorted_gene_std_dev_trt.png for comparison against rank_entropy_trt.png\n",
    "    - sorted exp error: (unrtrt-exp)\n",
    "        - gene_vs_meanerror.png vs. sorted_gene_vs_meanerror.png\n",
    "        - sorted_gene_vs_meanerror.png vs. rank_vs_avgerror_scatter.png\n",
    "\n",
    "MENTIONED:\n",
    "- look at benchmarks\n",
    "    - leo's; some bottleneck stuff?\n",
    "    - lea's; predicting gene expression from a different cell line and same perturbation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed1eccc",
   "metadata": {},
   "source": [
    "FOR REFERENCE:\n",
    "- MLP\n",
    "    - supervised learning (vs. AE is unsupervised)\n",
    "    - input layer + hidden layer(s) + output layer; loss is calcualted by comparing output to true label\n",
    "- DAE\n",
    "    - corrupts input data and decoder reconstructs the original; loss is calculated between reconstructed form z and original input x\n",
    "    - https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf (stacked DAE)\n",
    "    - https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf (robust ft w/ DAEs)\n",
    "- MAE\n",
    "    - in images: randomly mask 75% of an image, encoding only the visible patches and decoder reconstructs the original\n",
    "    - outside of images:\n",
    "        - https://arxiv.org/abs/2309.13793 (ReMasker)\n",
    "            - original input has missing values; additional inputs are masked for AE to train on - then applied to original missing values\n",
    "        - https://arxiv.org/abs/2412.19152 (PMAE)***\n",
    "            - masks original input w/ a probability inversely related to a column's observation rate\n",
    "            - ensures that the model must learn more from rarer/less frequently observed features\n",
    "            - more missing values = more mask (complete columns used for reconstruction)\n",
    "            - evaluated w/ coefficient of determination (numerical values) and accuracy (categorical values)\n",
    "            - argued that the MAE was better than the transformer version for local dependencies\n",
    "\n",
    "- CAE\n",
    "    - AE w/ small penalty to loss fxn to reduce sensitivity to small/local variations in the input\n",
    "\n",
    "aside:\n",
    "- https://arxiv.org/abs/2105.01601\n",
    "    - stacked MLP for computer vision that achieved competitive (around the same) scores compared to CNNs and vision transformers\n",
    "    - this is the model taht PMAE paper used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abab29b",
   "metadata": {},
   "source": [
    "sep 18, 2025 - todo\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298be776",
   "metadata": {},
   "source": [
    "INPUT COMPARISON:\n",
    "- complete reasoning results\n",
    "- complete error explanations\n",
    "- get graphs for exp 30ep (on smaug 0 rn)\n",
    "- begin run on rank 30ep afterwards\n",
    "\n",
    "ARCHITECTURE COMPARISON:\n",
    "- complete reasoning results\n",
    "- complete error explanations\n",
    "\n",
    "PLOTTING:\n",
    "- rank:\n",
    "- expression:\n",
    "        - double check the p(x) distribution of the bins; are they fitting in properly or are some bins have 0?\n",
    "- get correlation of avg hexbin for comparison in supplementary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1725fae",
   "metadata": {},
   "source": [
    "sep 20, 2025 - fixing 30ep runs, reorganize/planing for cp\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c3c7f1",
   "metadata": {},
   "source": [
    "INPUT COMPARISON:\n",
    "- complete reasoning results\n",
    "- complete error explanations\n",
    "- ~~get graphs for trt exp 30ep (30ep on smaug 1 rn thru sbatch)~~ done!\n",
    "- get graphs for trt rank 30ep (30ep on smaug 0 rn thru nohup)\n",
    "   - sbatch for rank doesn't work; some kind of OOM error?\n",
    "      - likely becasue the flag has to be --mem-per-gpu rather than --mem (smaug has 300gb total! or something)\n",
    "\n",
    "ARCHITECTURE COMPARISON:\n",
    "- build FNN-DAE for exp and rank\n",
    "   - running exp_nn --> issue with code; need to fix\n",
    "      - right now, it's kinda cooked. going to try to make it so that its reconstructing the original input rather than reconstructing the embedding space (output = 978 \\* batch rather than output = 64 \\* batch)\n",
    "      - should masking be done before compressing input into the embedding or after??\n",
    "      - things changed:\n",
    "         - removed output 0.0f0 when sum(mask) = 0 in the loss\n",
    "         - added mlp_head in Model + function\n",
    "      - or acc no need to reconstruct original.... becasue embebdding shoud already be a more robust represetnaton of the input data?\n",
    "      - should be fine now;\n",
    "         - just 1. increase mask ratio, 2. decrease LR, 3. normalize input?\n",
    "   - rank_nn still pending\n",
    "- complete reasoning results\n",
    "- complete error explanations\n",
    "\n",
    "PLOTTING:\n",
    "- rank:\n",
    "- expression:\n",
    "        - double check the p(x) distribution of the bins; are they fitting in properly or are some bins have 0?\n",
    "- get correlation of avg hexbin for comparison in supplementary\n",
    "\n",
    "GENERALLY:\n",
    "- create a doc of everything done, tests, etc. need to create some kind of story for talks later + poster presentations in oct\n",
    "- start prepping poster :0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff30eed",
   "metadata": {},
   "source": [
    "some additional things brought up last meeting:\n",
    "\n",
    "INPUT\n",
    "- exp:\n",
    "    - try w/ + w/o positional encoding\n",
    "    - ~~is posenc independent of gene exp?~~ YES.\n",
    "        - the positional encoding is calculated is deterministic;\n",
    "        - only uses the position and the embedding dimension index\n",
    "        - no knowledge of the actual gene expression values\n",
    "    - ~~or is posenc = gene exp embedding?~~ NO.\n",
    "        - posenc is added to gene exp embedding\n",
    "        - \"input .+ pe.pe_matrix[:,1:seq_len]\"\n",
    "- rank: \n",
    "    - is it possibel to pass (posenc + exp) --> add to rank?\n",
    "        - yes; if we were to make a hybrid version -- try later!!!********** via adding/concatenating rank embedding and expression projected into same dim as embedding\n",
    "    - ~~does rank need embeddings?~~\n",
    "        - if i were to feed raw rank integers, the model would assume rank 2 > rank 1 based on numerical distance between numbers (which is not true)\n",
    "        - since they are arbitrary ids, each rank should have a embedding vector that represents its identity and meaning\n",
    "    - ~~try embed_dim = 1; since rank input val = arbitrary #~~\n",
    "        - it isn't an arbitrary number! see above\n",
    "\n",
    "ARCH\n",
    "- tf:\n",
    "    - ~~output is either dummy embed vec returned from pretrain OR add/concat them together from 2nd last layer~~\n",
    "        - only need to output the embed vec from second to last layer in order to use weights for downstream tasks!!\n",
    "        - dummy embed vec = use classifier token [CLS] embed vec (BERT)\n",
    "            - put CLS @ beginning of every single input sequence before training\n",
    "            - the model's embedding layer learns a vector for this token just like any other token\n",
    "            - self-attention mechanism allows every token to see every other token, so the final hidden state corresponding to [CLS] becomes an aggregated summary of the entire sequence\n",
    "            - VIA: take first vector from the output tensor of final layer\n",
    "        - add/concat them tg = pooling embed vecs (GF)\n",
    "            - stop at the second-to-last layer, as GF says this layer has the most generalizable representations\n",
    "            - results in an embedding vector for every gene\n",
    "            - then pool these vectors into a single vector; via avg pooling or max pooling; concat would be too high dim!\n",
    "- nn:\n",
    "    - ~~is bottleneck layer needed?~~\n",
    "        - i have one! encoder = bottleneck (via shrinking down to latent space)\n",
    "- is it possibel to get tf to be as good as nn? (if tf not as good)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56781f08",
   "metadata": {},
   "source": [
    "oct 5, 2025 - fixing models for conference!\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf14d747",
   "metadata": {},
   "source": [
    "1. for profile embeddings (not gene based) change rank tf to either:\n",
    "    - CLS token (need to retrain)\n",
    "        - save train/test indices, fix model saving\n",
    "    - ~~mean pooling per sample across all embeddings at the end (no need to retrain)~~\n",
    "        - does not work because 1. did not save train/test indices, 2. model is saved in GPU mode, not CPU mode = cannot reload it in :(\n",
    "\n",
    "2. changed ranked input in AE to genes x samples iwth ranks noramlized between -1 and +1\n",
    "\n",
    "3. change evaluation to all the same (ranks!)\n",
    "\n",
    "4. avg hexbin and correlation for comparison/validation OR subtract mean from train/test (but would have to retrain...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6b641",
   "metadata": {},
   "source": [
    "nov 3, 2025 - todo for this wk\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef81880f",
   "metadata": {},
   "source": [
    "1. figure out why rank_tf isn't working well, with or without the CLS token addition.\n",
    "- likely due to the U-shape bias, common in MLM as a result of absolute positional encodings\n",
    "    - https://arxiv.org/abs/2502.01951 (reviewing why U-shape occurs + RPEs to solve)\n",
    "    - https://arxiv.org/abs/2307.03172 (initial finding of U-shape)\n",
    "- solutions include:\n",
    "    - cut top 200 genes such that we use seq_len != vocabulary (vocab>>> not just a permutation, but genuinely diff values)\n",
    "        - geneformer took 25k genes and cropped it to top 2048\n",
    "    - relative positional encodings\n",
    "        - https://arxiv.org/pdf/2403.04797\n",
    "        - https://arxiv.org/abs/1901.02860 \n",
    "    - separate content and position in attention calculations\n",
    "        - https://arxiv.org/abs/2006.03654 \n",
    "    - change in task to mask groups rather than individual tokens\n",
    "        - this supposedly allows the model to learn local context rather than global dependencies\n",
    "        - https://arxiv.org/abs/1907.10529 (SpanBERT)\n",
    "2. change evaluation to the same; since everything except rank tf is a regression task, make rank tf predict expression values?\n",
    "3. add downstream task? maybe it just needs to be on a downstream task\n",
    "- it has been said that loss, or MLM accuracy does not equate to high downstream task applications\n",
    "- ex.\n",
    "    - https://arxiv.org/abs/1907.10529 (SpanBERT)\n",
    "        - outperforms BERT on downstream but lower MLM accuracy\n",
    "    - https://arxiv.org/abs/2210.14199 (loss vs. downstream)\n",
    "        - also says that flatter minima is better, rather than lower\n",
    "    - https://arxiv.org/abs/2410.08455 \n",
    "        - says that during fine-tuning, a lot of pretrain info is discarded; thus as long as core info remains (regardless of MLM score), it is useful\n",
    "- potential tasks:\n",
    "    - **drug response prediction (GDSC or CCLE cell line data to becnhmark)**\n",
    "        - given a patient's exp data and a drug, will the pateints tumor respond to the drug?\n",
    "    - perturbation modelling (zero-shot, used in GF)\n",
    "        - if i knock out gene x, what will happen to cell expression?\n",
    "    - gene regulatory network inference (used in scBERT for scRNA-seq)\n",
    "        - which genes regulate other genes via model attn weights?\n",
    "    - fine tune first, zero shot can come later.\n",
    "- for drug response prediction:\n",
    "    - https://pmc.ncbi.nlm.nih.gov/articles/PMC7737474/ (GNN using a VAE on CCLE to predict drug response from GDSC)\n",
    "    - https://pmc.ncbi.nlm.nih.gov/articles/PMC7319576/ (attn w/ SMILES and exp to predict CCL sensitivity)\n",
    "    - https://pmc.ncbi.nlm.nih.gov/articles/PMC6612815/ (multi-omics integration for drug response prediction)\n",
    "- WHAT IF I INPUT A LITERAL STRING INSTEAD OF NUMBERS. THUS LEARNING THE ACTUAL GENE ? THEN IT WONT GET CONFUSED\n",
    "\n",
    "for ppt:\n",
    "- review tabular data issues in transformers, math, etc.\n",
    "- is there any way to leverage or alleviate it?\n",
    "\n",
    "currently running:\n",
    "- smaug via sbatch\n",
    "    - 200_rank_tf.jl, 100ep since 200 ran very well first time (lower error w/10ep than regular run 10ep)\n",
    "- smaug via sbatch\n",
    "    - rank_tf.jl, 1 layer since testing claim that U-shaped bias results from compounding attention layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3343fe59",
   "metadata": {},
   "source": [
    "plan:\n",
    "--\n",
    "baseline: rank_tf.jl, untrt/rank_tf/2025-11-05_00-36 --- DONE\n",
    "\n",
    "cut seq_len to 200: 200_rank_tf.jl, untrt/rank_tf/2025-11-05_13-56 --- DONE\n",
    "- does better than original!\n",
    "\n",
    "\" but longer run (10ep --> 100ep): 200_rank_tf.jl, untrt/rank_tf/2025-11-05_17-13 --- DONE\n",
    "- mega overfitting but still does better than original...\n",
    "\n",
    "\" but with smaller model: 200_rank_tf.jl, untrt/rank_tf/2025-11-05_17-13 --- DONE\n",
    "- still overfitting but doesn't do better than other model\n",
    "\n",
    "cut n_layers (4 --> 1): rank_tf.jl, untrt/rank_tf/2025-11-05_15-48 --- DONE\n",
    "- does not fix U-shape bias\n",
    "\n",
    "try ALiBi: rpe_rank_tf.jl, untrt/rank_tf/2025-11-06_06-01 --- DONE\n",
    "- double check code; its doing worse than baseline...?\n",
    "\n",
    "try RoPE: rpe_rank_tf.jl,\n",
    "\n",
    "eval on downstream task: task_rank_tf.jl,\n",
    "- baseline: predict sensitivity for (drug, cell_line) pair\n",
    "- harder: LDO (drug-blind) or LCO (cell-blind)\n",
    "- rank AE vs. rank TF\n",
    "- exp TF vs. rank TF\n",
    "\n",
    "make rank tf predict expression values (regression task):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685009b9",
   "metadata": {},
   "source": [
    "nov 12, 2025 - post seb meeting\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d64b6",
   "metadata": {},
   "source": [
    "regarding the Potential Explanations:\n",
    "1. seq_len != vocab_size\n",
    "    - seq_len decr from 978 --> 200\n",
    "        - do longer run anyways\n",
    "        - if overfitting, decr # params, incr regularization (like dropout)\n",
    "        - need full log of \"what was tried\" and \"why it didn't work\"\n",
    "    - then can sufficiently say this is not the case.\n",
    "2. U-shape bias\n",
    "    - hypothesis A: converting from exp to rank introduces more noise in middle ranks\n",
    "        - wherein, irrelevant small changes in exp result in high changes in rank\n",
    "        - plot:\n",
    "            - variance(exp) vs. rank\n",
    "            - predicted vs. true ranks' DIFFERENCE in expression vs. rank\n",
    "            - validate w/ plotting cross entropy loss instead of error against rank\n",
    "                - see which ranks the loss struggles w/, if this is the same as the mean predicted error vs. rank plot or if there is a tradeoff somewhere\n",
    "                - do carl vid on this heheh\n",
    "    - hypothesis B: positional encodings introduce some of this bias\n",
    "        - plot:\n",
    "            - matrix of sum of squared differences between positional encodings\n",
    "                - expect lower distances in the middle\n",
    "    - hypothesis C: attention layers compounding introduce this bias\n",
    "        - this cannot be true (?) as this was introduced for CAUSAL (autoregressive, one direction) masks.\n",
    "        - can fact check this tho\n",
    "    - if we conclude that this is a product of rankings+transformers (hypothesis A is true):\n",
    "        - next steps:\n",
    "            - change in dataset; determine if this is LINCS-dependent (although realistically a foundational model should work with all types of datasets...)\n",
    "            - is it possible to convert expression to sequential data without ranking?\n",
    "                - can look into using the difference in expression instead of rank\n",
    "                    - but then we lose the positional information.\n",
    "                    - maybe could do MSE over the expression between predicted and true gene as the loss function\n",
    "            - inductive bias (maybe not worth the effort tho)\n",
    "                - inform model w/ what is already seen via pushing logits to 0 or if entropy is high, increase the attention given to those tokens.\n",
    "3. MLM accuracy not important\n",
    "    - not important right now, but:\n",
    "        - compare CLS token with embedding space in autoencoder (for fine tuning later)\n",
    "    - for now, lets focus on imporving masking accuracy :\n",
    "4. eval is diff b/n comparisons\n",
    "    - options:\n",
    "        - convert rank AE to classification (for proper comparison); should be easy\n",
    "        - change evaluation of rank TF to MSE (?how)\n",
    "            - make the positional encodings = expression values, then have the TF predict expression values\n",
    "                - inject PE at different points of the TF (ex. after each layer)\n",
    "                - so either:\n",
    "                    - PE = expression vals, or\n",
    "                    - PE = change in expression vals\n",
    "    - here's a less important hypothesis:\n",
    "        - rank TF is not seeing the input as sequential, but rather tabular\n",
    "            - set PE = 0 to see if learning changes...? \n",
    "            - or ..?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28fa600",
   "metadata": {},
   "source": [
    "nov 24 - todo: u-shape bias + diff eval\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505ee2c",
   "metadata": {},
   "source": [
    "testing:\n",
    "- u-shape bias\n",
    "    - **hypothesis A: converting from exp to rank introduces more noise in middle ranks**\n",
    "        - plot:\n",
    "            - ~~variance(exp) vs. rank~~\n",
    "            - ~~predicted vs. true ranks' DIFFERENCE in expression vs. rank~~\n",
    "            - ~~validate w/ plotting cross entropy loss instead of error against rank~~\n",
    "                - ~~see which ranks the loss struggles w/, if this is the same as the mean predicted error vs. rank plot or if there is a tradeoff somewhere~~ same\n",
    "                - do carl vid on this heheh\n",
    "    - **hypothesis B: positional encodings introduce some of this bias**\n",
    "        - plot:\n",
    "            - ~~matrix of sum of squared differences between positional encodings~~\n",
    "                - expect lower distances in the middle\n",
    "- eval is diff b/n comparisons\n",
    "    - **hypothesis A: evaluation**\n",
    "        - check:\n",
    "            - convert rank AE to classificcation\n",
    "            - change eval of rank TF to MSE\n",
    "                - PE = exp vals or PE = change in exp vals\n",
    "    - **hypothesis B: data is being read in as tabular not sequential**\n",
    "        - check:\n",
    "            - set PE = 0 to see if learning changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b14a591",
   "metadata": {},
   "source": [
    "nov 26 - post-seb meeting: what to do abt middle rank variance...\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128169a",
   "metadata": {},
   "source": [
    "HYPOTHESIS A: converting from exp to rank introduces noise in middle, with small changes in exp = large changes in rank\n",
    "- we can sufficiently conclude that the largest contributor to this u-shape bias is this via:\n",
    "    - PLOT 1:\n",
    "        - checking if model predicts wrong gene but predicted gene has siilar exp to true gene\n",
    "        - ~~TODO:~~\n",
    "            - do not absolute val it; look at neg too!\n",
    "            - smaller markers so its not just a blob on the bottom (or could 2D hist or x-bin...)\n",
    "            - need mean or median line (to see if its following the same trend as the variance plot)\n",
    "    - PLOT 2/3:\n",
    "        - checking variance in exp in dataset and predicted models\n",
    "        - we would expect here that its easier to predict the middle due to the low variance\n",
    "        - ~~TODO:~~\n",
    "            - superimpose the predicted scatter plot and the true line plot onto one another\n",
    "            - could do loess smoothing but prolly no need\n",
    "    - PLOT 4:\n",
    "        - looking at plot 1, we can see that the model knows what the exp value should be (in the ballpark) but has difficulty predicting the actual identity via rank in plot 4!!!\n",
    "        - larger contribution to loss = more impactful it is to do well in edge cases\n",
    "        - better reward to take care of big errors (aka lower the top loss (the values around 10-20) of the scatter down rather than focusing on making the bottom part closer to 0)\n",
    "    - PLOT 5: NOT IMPORTANT\n",
    "- NEXT STEPS:\n",
    "    - change in dataset; is this seen in RNA seq?\n",
    "        - if data has this property, then which model/architecture would be better?\n",
    "        - ~~distribution of exp in LINCS vs. TCGA~~\n",
    "            - GTEX is not good because most tissue comes from same person\n",
    "            - if it is similar in concentration (distribution of variance), then it will likely be the same result\n",
    "    - converting exp to seq w/o ranking via change in exp?\n",
    "        - could look into parallel sorting via DAG but this is local ordering rather than global\n",
    "        - exp val as posenc!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b8486",
   "metadata": {},
   "source": [
    "nov 30 - post tcga analysis\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9547e8d3",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- PLOT 4:\n",
    "    - looking at plot 1, we can see that the model knows what the exp value should be (in the ballpark) but has difficulty predicting the actual identity via rank in plot 4!!!\n",
    "    - larger contribution to loss = more impactful it is to do well in edge cases\n",
    "    - better reward to take care of big errors (aka lower the top loss (the values around 10-20) of the scatter down rather than focusing on making the bottom part closer to 0)\n",
    "    - HOW???\n",
    "    - **CURRENTLY:**\n",
    "        - long ass run (200ep on untrt) to see if it gets rid of bigger errors + hits a plateau?\n",
    "        - some ideas:\n",
    "            - rank-weighted loss\n",
    "                - place higher prio on initial ranks\n",
    "                - i don't really like this, i want to assume everything is important\n",
    "            - gaussian soft labels\n",
    "                - create a gaussian distr centered on the true rank\n",
    "                - rather than strict 0-1 penalties, higher penalty the further it is away from true rank\n",
    "                - also don't really like this, but could consider...\n",
    "            - auxillary regression\n",
    "                - output two things: logits (rank) and scalar (exp)\n",
    "                - loss = CE loss + (lambda*MSE loss)\n",
    "                - do this first!\n",
    "            - add expression to embedding vector \n",
    "                - add 0.0 as exp for masked values\n",
    "                - do this second!\n",
    "\n",
    "- TCGA:\n",
    "    - has same distr (mean exp, entropy, variance) by rank as LINCS... further analysis warranted? probably not\n",
    "    - if time, can do testing with pre-trained model?? or train...? \n",
    "    - **CURRENTLY:**\n",
    "        - not important rn\n",
    "    \n",
    "- RANKING METHOD:\n",
    "    - converting exp to seq w/o ranking via change in exp?\n",
    "    - could look into parallel sorting via DAG but this is local ordering rather than global\n",
    "    - exp val as posenc!!!\n",
    "    - **CURRENTLY:**\n",
    "        - ?\n",
    "\n",
    "- WHY ARE THE RESULTS FROM NO POSENC AND WITH POSENC THE SAME??\n",
    "    - DOES THIS MEAN ITS NOT BEING READ AS SEQUENTIAL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77f1f26",
   "metadata": {},
   "source": [
    "dec 2, 2025 - evaluating ranking method\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2e22a",
   "metadata": {},
   "source": [
    "IMPROVING RANKING MODEL\n",
    "- ~~long run (200ep)~~ **on smaug 0 (tbd thurs) - job id 632538, pid 2725147**\n",
    "- to match GF:\n",
    "    - ~~normalize exp in each cell by avg exp across dataset~~ **on smaug 1 (100e, tbd thurs) - job id 632663, pid 2930860**\n",
    "    - ~~learned posenc instead of sinusoidal~~ **on smaug 3, (100e, tbd thurs) - pid 30378148**\n",
    "- ~~verify sequential input:~~\n",
    "    - why are the results from no posenc and with posenc the same?\n",
    "    - verified with cosine similarity of CLS embedding vectors\n",
    "- to incorporate exp:\n",
    "    - ~~a. add expression to embedding vector~~ **on kraken 1 (full 30ep) - pid 2751262, tbd thurs started wed 1230**\n",
    "        - add 0.0 as exp for masked values\n",
    "        - input = embed(geneID) + embed(exp_bin)\n",
    "        - similar to what scGPT and Tx1 do; but with ranks...? not sure if this is repeating the same info as ranks tho? or it's just the same thing as using expression value?\n",
    "        - i think you have to do it with bins tho\n",
    "    - ~~b. regression head~~ **on kraken 0 (full 30ep) - pid 2751432, tbd thurs (started wed 1230)**\n",
    "        - output two things: logits (rank) and scalar (exp) via both a rank head and exp head\n",
    "        - loss = CE loss + (lambda*MSE loss)\n",
    "        - essentially guides rank prediction to correct magnitude as well\n",
    "\n",
    "CHANGING RANKING MODEL\n",
    "- converting exp to seq w/o ranking:\n",
    "    - standards:\n",
    "        - a. binning\n",
    "            - this is what scGPT and Tx1 currently use\n",
    "            - similar to using exp val as the positional encoder\n",
    "            - input = embed(geneID) + embed(exp_bin) + embed(mask)\n",
    "            - and remove the positional encoding part\n",
    "        - b. continuous value projection\n",
    "            - input = embed(geneID) + float(exp_val)\n",
    "            - im pretty sure this is very similar to what i did for the exp value transformer\n",
    "            - but just used float(exp-val) instead of with embed(geneID)\n",
    "    - maybe try:\n",
    "        - use change in exp as the rank?\n",
    "        - could look into parallel sorting via DAG but this is local ordering rather than global\n",
    "\n",
    "CHANGING DATASET:\n",
    "- if time, can test pre-trained model on TCGA or retrain on TCGA\n",
    "- although, since it has the same distr as LINCS it likely won't work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687d6da7",
   "metadata": {},
   "source": [
    "note: is it recommended to use continuous rather than binning for high depth data like bulk RNA seq or pseudobulk???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdc71b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see which file is running, in terminal:\n",
    "# using SLURM:\n",
    "    # scontrol show job 632661 | grep Command for the path to executable/script\n",
    "    # scontrol show job 12345 for full\n",
    "    # squeue -j 12345 -o \"%.1000j\"\n",
    "    # scontrol write batch_script 12345 - for the file itself\n",
    "# using nohup:\n",
    "    # ps -fp 12345 for script\n",
    "    # ps -p 12345 -o args for wrap\n",
    "    # cat /proc/12345/cmdline for all running processes\n",
    "    # ls -l /proc/12345/cwd for folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd211134",
   "metadata": {},
   "source": [
    "note need to redo normalization code; its not using the nroamzalized data i htink\n",
    "don't absolute value the prediction error rank and maybe dont mean it???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa64594",
   "metadata": {},
   "source": [
    "dec 5, 2025 - todo post-meeting\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6f9ba",
   "metadata": {},
   "source": [
    "MATCHING GENEFORMER\n",
    "- redo global normalization code; its not using the normalized data\n",
    "- show entropy, etc. plots of rank matrix after normalization; shouldn't be the U-shape if its been normalized like that... right?\n",
    "- need to accordingly normalize the input for the expression model too for comparison\n",
    "- can do learned posenc, however this likely will not make a difference (carl says there is literature on it not affecting much)\n",
    "- likely tho this won't be the primary cause of the issue\n",
    "\n",
    "SEQUENTIAL VS. TABULAR\n",
    "- verify what is tabular? what is sequential?\n",
    "- from these results, we can conclude that it is being seen as sequential (specifically due to CLS embeddign plot)\n",
    "- any way to make it more understandable? might not be relevant tho\n",
    "- maybe try concat instead of addition and see if that changes anything lol\n",
    "\n",
    "EVALUATING PREDICTIONS\n",
    "- need *** plot variance by average expression (0-15) to see if it matches the distribution of the variance by rank plot\n",
    "- can conclude from the results that the hypothesis is true, however what lea says is that the variance by rank plot is incorrect?\n",
    "\n",
    "LINCS VS. TCGA\n",
    "- can do TCGA because this is more similar to actual RNA seq we will be using later\n",
    "- if there is a discrepancy between LINCS and TCGA resutls, then the result is readout-specific\n",
    "- don't need to test EVERYTHING on tcga, just the results that are noteworthy from LINCS\n",
    "\n",
    "INCORPORATING EXP VALUES\n",
    "- carl asked: isn't the point of doing rank so that we don't have to use expression values??\n",
    "\n",
    "for seb meeting:\n",
    "- suggestions for CP presentation?\n",
    "- what can i be doing better as a student/researcher? (maybe aritculation...)\n",
    "- i feel behind"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
